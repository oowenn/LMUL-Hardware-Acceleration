{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae58495f-7be2-4969-9c39-e49a5ac35890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lmul_nn_funcs import lmul_bits \n",
    "def lmul(a, b, M=7):\n",
    "    return lmul_bits(a, b)\n",
    "\n",
    "#this is the dataset loader\n",
    "#this is used for regressive generation;\n",
    "\n",
    "#needed heavy LLM help to build this notebook out\n",
    "class CharDatasetFixed(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, pad_char=\" \", seq_len=64):\n",
    "        self.pad_char = pad_char\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # include pad_char explicitly\n",
    "        all_text = \"\".join(sentences) + pad_char\n",
    "        chars = sorted(list(set(all_text)))   # now pad_char is in chars\n",
    "        self.stoi = {c:i for i,c in enumerate(chars)}\n",
    "        self.itos = {i:c for c,i in self.stoi.items()}\n",
    "        self.vocab = len(chars)\n",
    "\n",
    "        # convert sentences to indices and pad\n",
    "        self.data = []\n",
    "        for s in sentences:\n",
    "            s_fixed = s[:seq_len].ljust(seq_len, pad_char)\n",
    "            self.data.append(torch.tensor([self.stoi[c] for c in s_fixed]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.data[idx]\n",
    "        x = s[:-1]  # input sequence\n",
    "        y = s[1:]   # target = next character\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302a7a1d-4c5d-4dce-9fc5-f80351208144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "class LSTMLayerLMUL(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.M = M\n",
    "        self.W = nn.Linear(input_size + hidden_size, 4*hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        combined = torch.cat((x_t, h_prev), dim=1)\n",
    "\n",
    "        if self.use_lmul:\n",
    "            W = self.W.weight\n",
    "            b = self.W.bias\n",
    "            B, I = combined.shape\n",
    "            O, _ = W.shape\n",
    "            prod = lmul(combined.unsqueeze(1), W.unsqueeze(0), M=self.M)\n",
    "            gates = prod.sum(dim=2) + b\n",
    "        else:\n",
    "            gates = self.W(combined)\n",
    "\n",
    "        i, f, g, o = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        if self.use_lmul:\n",
    "            c_t = lmul(f, c_prev, M=self.M) + lmul(i, g, M=self.M)\n",
    "            h_t = lmul(o, torch.tanh(c_t), M=self.M)\n",
    "        else:\n",
    "            c_t = f * c_prev + i * g\n",
    "            h_t = o * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd8793d-2b74-48aa-ace0-3071b36234eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLMUL_LLM(nn.Module):\n",
    "    def __init__(self, vocab, hidden=128, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden\n",
    "        self.use_lmul = use_lmul\n",
    "\n",
    "        self.embed = nn.Embedding(vocab, hidden)\n",
    "        self.lstm = LSTMLayerLMUL(hidden, hidden, use_lmul, M)\n",
    "        self.fc = nn.Linear(hidden, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        h = torch.zeros(B, self.hidden_size)\n",
    "        c = torch.zeros(B, self.hidden_size)\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = self.embed(x[:, t])\n",
    "            h, c = self.lstm(x_t, h, c)\n",
    "\n",
    "        logits = self.fc(h)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3194a850-bab3-4c5d-9f8e-910a3e5e9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs=5, lr=3e-4):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "        for x, y in loader:\n",
    "            opt.zero_grad()\n",
    "            logits = model(x)\n",
    "\n",
    "            loss = F.cross_entropy(logits, y[:, -1])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, loss={total/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f21edd5-4311-437c-b99f-e080119c7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, dataset, start=\"A\", length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    chars = [dataset.stoi[c] for c in start]\n",
    "    h = torch.zeros(1, model.hidden_size)\n",
    "    c = torch.zeros(1, model.hidden_size)\n",
    "    # prime with prefix\n",
    "    for ch in chars:\n",
    "        x_t = model.embed(torch.tensor([ch]))\n",
    "        h, c = model.lstm(x_t, h, c)\n",
    "\n",
    "    out = start\n",
    "    token = chars[-1]\n",
    "\n",
    "    for _ in range(length):\n",
    "        logits = model.fc(h) / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        out += dataset.itos[token]\n",
    "\n",
    "        x_t = model.embed(torch.tensor([token]))\n",
    "        h, c = model.lstm(x_t, h, c)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b571c80-98d5-485c-8c8d-e482350f7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text\n",
    "sentences = text.split(\"\\n\")\n",
    "new_sentences = []\n",
    "for i in range(0, len(sentences), 5):\n",
    "    #every 5 sentences, I merge\n",
    "    new_sentences.append(\"\\n\".join(sentences[i:i+5]))\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8209026-8f8a-45a7-a4f0-d0366271f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8001"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4718183-1930-4843-b93a-0d87e661bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=1.5681\n",
      "Epoch 2, loss=1.2343\n",
      "Epoch 3, loss=1.1185\n",
      "Epoch 4, loss=1.0215\n",
      "Epoch 5, loss=0.9313\n",
      "Epoch 6, loss=0.8508\n",
      "Epoch 7, loss=0.7740\n",
      "Epoch 8, loss=0.6973\n",
      "Epoch 9, loss=0.6265\n",
      "Epoch 10, loss=0.5535\n"
     ]
    }
   ],
   "source": [
    "#we dont need to run this cell anymore; its JUST for trainin and we save the weights anyways\n",
    "dataset = CharDatasetFixed(new_sentences[0:1000], pad_char=\" \", seq_len=128)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#Standard FP32 LSTM (we dont use LMUL to train because gradienting using operations in LMUL is crazy cringe and not possible)\n",
    "model_fp32 = TinyLMUL_LLM(dataset.vocab, hidden=128, use_lmul=False)\n",
    "train(model_fp32, loader, epochs=10)\n",
    "\n",
    "#Save the FP32 weights\n",
    "torch.save(model_fp32.state_dict(), \"tiny_llm_fp32.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f689c151-dd96-404e-90e3-f34b1c8cecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exquisite fr                                                                                                                                                                                                                                                                                                         \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Exquisite\"\n",
    "print(generate(model_fp32, dataset, start=prompt, length=300, temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcdb6992-909d-42ca-88c8-649100fe98bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyLMUL_LLM(\n",
       "  (embed): Embedding(61, 128)\n",
       "  (lstm): LSTMLayerLMUL(\n",
       "    (W): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=61, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lmul = TinyLMUL_LLM(dataset.vocab, hidden=128, use_lmul=True)\n",
    "\n",
    "#Load the FP32-trained weights, but we use the LSTM with LMUL operation\n",
    "model_lmul.load_state_dict(torch.load(\"tiny_llm_fp32.pth\"))\n",
    "model_lmul.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2286881-3257-456b-8aa0-a7ff0388a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LMUL LSTM output:\n",
      " Caiustand the for                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "lmul_text = generate(model_lmul, dataset, start=prompt, length=300, temperature=0.01)\n",
    "print(\"\\nLMUL LSTM output:\\n\", lmul_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a6374-18f7-4033-9cd6-cedbc47b58ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
