{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60afc2ee-d948-4e4a-aab3-15775c53a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Notebook folder: /workspace/mnistmlptest/LSTM_Verilog\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Convenience: device choice\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#for isolating the verilog\n",
    "#new_dir = \"../mnistmlptest/LSTM_Verilog\"\n",
    "#os.chdir(new_dir)\n",
    "notebook_dir = os.getcwd()  \n",
    "print(\"Notebook folder:\", notebook_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b111acd-679b-4d7b-9b66-5690d6e4b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmul_bits(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    #same LMUL bits\n",
    "    a = a.to(torch.float32)\n",
    "    b = b.to(torch.float32)\n",
    "\n",
    "    a_bits = a.view(torch.int32)\n",
    "    b_bits = b.view(torch.int32)\n",
    "\n",
    "    a_bf16 = (a_bits >> 16) & 0xFFFF\n",
    "    b_bf16 = (b_bits >> 16) & 0xFFFF\n",
    "\n",
    "    a_sign = (a_bf16 >> 15) & 0x1\n",
    "    b_sign = (b_bf16 >> 15) & 0x1\n",
    "\n",
    "    a_field = a_bf16 & 0x7FFF\n",
    "    b_field = b_bf16 & 0x7FFF\n",
    "\n",
    "    a_exp = (a_field >> 7) & 0xFF\n",
    "    b_exp = (b_field >> 7) & 0xFF\n",
    "    zero_mask = (a_exp == 0) | (b_exp == 0)\n",
    "\n",
    "    OFFSET_MOD = 0x4080  \n",
    "    sum_full = a_field.to(torch.int32) + b_field.to(torch.int32) + OFFSET_MOD\n",
    "\n",
    "    carry2 = (sum_full >> 15) & 0x3\n",
    "\n",
    "    mask_underflow = (carry2 == 0)\n",
    "    mask_normal    = (carry2 == 1)\n",
    "    mask_overflow  = (carry2 >= 2)\n",
    "\n",
    "    field_sel = torch.zeros_like(sum_full)\n",
    "    field_sel = torch.where(mask_normal, sum_full & 0x7FFF, field_sel)\n",
    "    field_sel = torch.where(mask_overflow, torch.tensor(0x7FFF, dtype=torch.int32, device=sum_full.device), field_sel)\n",
    "\n",
    "    s_result = (a_sign ^ b_sign).to(torch.int32)\n",
    "    s_result = torch.where(field_sel == 0, torch.tensor(0, device=sum_full.device, dtype=torch.int32), s_result)\n",
    "\n",
    "    result_bits_bf16 = ((s_result << 15) | field_sel).to(torch.int32)\n",
    "    result_bits_f32 = result_bits_bf16 << 16\n",
    "    result = result_bits_f32.view(torch.float32)\n",
    "\n",
    "    # bias correction you added\n",
    "    result = result + (result / (1 << 5)) + (result / (1 << 6))\n",
    "    result = torch.where(zero_mask, torch.zeros_like(result), result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def lmul(a, b, M=7):\n",
    "    return lmul_bits(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "389f05a6-8e55-488a-b117-3fa3d3dd4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37139bf8-bc46-465a-b2d4-3bc856433d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data loaders + LSTM code (identical to yours)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.M = M\n",
    "        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        combined = torch.cat((x_t, h_prev), dim=1)\n",
    "        if self.use_lmul:\n",
    "            W = self.W.weight\n",
    "            b = self.W.bias\n",
    "            x_exp = combined.unsqueeze(1)\n",
    "            W_exp = W.unsqueeze(0)\n",
    "            prod = lmul(x_exp, W_exp, M=self.M)\n",
    "            gates = prod.sum(dim=2) + b\n",
    "        else:\n",
    "            gates = self.W(combined)\n",
    "        i, f, g, o = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i); f = torch.sigmoid(f); o = torch.sigmoid(o); g = torch.tanh(g)\n",
    "        if self.use_lmul:\n",
    "            c_t = lmul(f, c_prev, M=self.M) + lmul(i, g, M=self.M)\n",
    "            h_t = lmul(o, torch.tanh(c_t), M=self.M)\n",
    "        else:\n",
    "            c_t = f * c_prev + i * g\n",
    "            h_t = o * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=28, hidden_size=128, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.lstm_cell = LSTMLayer(input_size, hidden_size, use_lmul, M)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.squeeze(1)\n",
    "        h = torch.zeros(B, self.hidden_size)\n",
    "        c = torch.zeros(B, self.hidden_size)\n",
    "        for t in range(28):\n",
    "            x_t = x[:, t, :]\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "        out = self.fc(h)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "def train_model(model, optimizer, loader, epochs=2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += len(target)\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5ff489-2a65-42c8-b52f-fc9d847c874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model (this may take a bit)...\n",
      "Done training.\n",
      "Baseline accuracy: 96.71%\n",
      "W shape: torch.Size([512, 156])\n",
      "Combined vector shape: torch.Size([1, 912])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: train baseline and extract weights + one sample input vector\n",
    "model = LSTMClassifier(use_lmul=False).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training baseline model (this may take a bit)...\")\n",
    "train_model(model, opt, train_loader, epochs=3)  # reduce epochs if you want\n",
    "print(\"Done training.\")\n",
    "\n",
    "# Test baseline accuracy (optional)\n",
    "baseline_acc = test_acc(model, test_loader)\n",
    "print(f\"Baseline accuracy: {baseline_acc:.2f}%\")\n",
    "\n",
    "# Extract W matrix from the LSTM layer (shape [4*H, I+H])\n",
    "W = model.lstm_cell.W.weight.detach().cpu().clone()  # torch.Tensor [O, I]\n",
    "O, I_plus_H = W.shape\n",
    "print(\"W shape:\", W.shape)  # expect [512, 156] for H=128, input_size=28\n",
    "\n",
    "# Prepare a single input vector `combined` = [x_row, h_prev] for a single timestep\n",
    "# We'll just pick the first test batch and first timestep's combined vector (x_row concatenated with zeros h_prev)\n",
    "# Build combined vector: x_t (28 dims) and h_prev zeros (128 dims) => length 156\n",
    "sample_data, _ = next(iter(test_loader))  # batch of size 1000\n",
    "sample_data = sample_data[0:1]  # pick first example\n",
    "x_rows = sample_data.squeeze(1)  # [28, 28]\n",
    "x_row0 = x_rows[0]               # shape [28]\n",
    "x_row0 = x_row0.flatten()        # ensure shape [28]\n",
    "\n",
    "h0 = torch.zeros(128)            # shape [128]\n",
    "\n",
    "combined = torch.cat((x_row0, h0), dim=0)  # shape [156]\n",
    "combined = combined.unsqueeze(0)           # [1, 156]\n",
    "print(\"Combined vector shape:\", combined.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d278ef6-6176-4f8b-9084-1992a35ef23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_bf16.txt, x_bf16.txt, fp32_products.txt written.\n",
      "Total multiplies = 79872\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: write memory files for Verilog\n",
    "def float_to_bf16_upper16_bits(f):\n",
    "    # f: python float or numpy float\n",
    "    i = struct.unpack(\">I\", struct.pack(\">f\", float(f)))[0]\n",
    "    return (i >> 16) & 0xFFFF  # uint16\n",
    "\n",
    "def float_to_u32bits(f):\n",
    "    return struct.unpack(\">I\", struct.pack(\">f\", float(f)))[0]\n",
    "\n",
    "# Prepare directories/files\n",
    "open(\"W_bf16.txt\", \"w\").close()\n",
    "open(\"x_bf16.txt\", \"w\").close()\n",
    "open(\"fp32_products.txt\", \"w\").close()\n",
    "\n",
    "# Flatten W row-major and write BF16 upper16 hex per line\n",
    "W_np = W.numpy()\n",
    "rows, cols = W_np.shape\n",
    "with open(\"W_bf16.txt\", \"w\") as fW:\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            val = W_np[r, c]\n",
    "            bf16 = float_to_bf16_upper16_bits(val)\n",
    "            fW.write(f\"{bf16:04x}\\n\")\n",
    "\n",
    "# Write x vector BF16 upper16\n",
    "x_np = combined.squeeze(0).numpy()\n",
    "with open(\"x_bf16.txt\", \"w\") as fx:\n",
    "    for k in range(len(x_np)):\n",
    "        bf16 = float_to_bf16_upper16_bits(x_np[k])\n",
    "        fx.write(f\"{bf16:04x}\\n\")\n",
    "\n",
    "# Precompute FP32 products for FP32 path: for each W_ij * x_j\n",
    "# (we will write them in the same multiplication order: for row r in 0..rows-1, for col c in 0..cols-1)\n",
    "with open(\"fp32_products.txt\", \"w\") as fprod:\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            prod = float(W_np[r, c]) * float(x_np[c])\n",
    "            prod_bits = float_to_u32bits(prod)\n",
    "            fprod.write(f\"{prod_bits:08x}\\n\")\n",
    "\n",
    "print(\"W_bf16.txt, x_bf16.txt, fp32_products.txt written.\")\n",
    "print(f\"Total multiplies = {rows*cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71764100-a437-40c7-9a78-5062bfd38bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Verilog...\n",
      "Running simulation (vvp)...\n"
     ]
    }
   ],
   "source": [
    "## Construct full paths to your Verilog files\n",
    "lmul_file = \"LMUL_LSTM.v\"\n",
    "tb_file = \"TB_WX.v\"\n",
    "fp32 = \"FP32.v\"\n",
    "sim_out = os.path.join(notebook_dir, \"sim.out\")\n",
    "\n",
    "\n",
    "print(\"Compiling Verilog...\")\n",
    "proc = subprocess.run([\"iverilog\", \"-g2012\", \"-o\", sim_out, lmul_file, fp32, tb_file],capture_output=True, text=True)\n",
    "if proc.returncode != 0:\n",
    "    print(\"iverilog compilation failed:\")\n",
    "    print(proc.stderr)\n",
    "else:\n",
    "    print(\"Running simulation (vvp)...\")\n",
    "    proc = subprocess.run([\"vvp\", \"sim.out\"], capture_output=True, text=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"Simulation failed:\")\n",
    "        print(proc.stderr)\n",
    "    else:\n",
    "        print(\"Simulation finished, outputs written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648929dd-e1d3-4e89-9caa-db8ae3636ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "fp32_products.txt\n",
      "lmul_bf16.v\n",
      "lmul_tester.py\n",
      "MNIST\n",
      "py_lmul.py\n",
      "sim.out\n",
      "simple_function.v\n",
      "top_lmul.v\n",
      "W_bf16.txt\n",
      "x_bf16.txt\n",
      "__pycache__\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for f in os.listdir(\".\"):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b351ebd4-11ee-48d9-a8da-439d1d021243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook folder: /workspace/mnistmlptest/LSTM_Verilog\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eba8627-818a-49ba-9d2b-2abddf98ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists? True\n",
      "Contents of folder:\n",
      ".ipynb_checkpoints\n",
      "fp32_products.txt\n",
      "LMUL_LSTM.v\n",
      "lstm_verilog_tester.ipynb\n",
      "sim.out\n",
      "TB_WX.v\n",
      "verilog_checksums.txt\n",
      "W_bf16.txt\n",
      "x_bf16.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder = \"../mnistmlptest/LSTM_Verilog\"\n",
    "print(\"Folder exists?\", os.path.exists(folder))\n",
    "print(\"Contents of folder:\")\n",
    "for f in os.listdir(folder):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89f95a-37db-49d3-b570-d459e9e471ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
