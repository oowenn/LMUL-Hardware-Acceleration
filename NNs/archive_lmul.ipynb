{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT IS THIS?\n",
    "This is the lmul.ipynb notebook uploaded in #things channel, but just pushed onto the github for comparison reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hardware_accelerators.dtypes import *\n",
    "from hardware_accelerators.rtllib.lmul import *\n",
    "from hardware_accelerators.rtllib.multipliers import *\n",
    "from hardware_accelerators.rtllib.adders import *\n",
    "from hardware_accelerators.rtllib.utils.lmul_utils import *\n",
    "from hardware_accelerators.simulation.utils import render_waveform\n",
    "from hardware_accelerators.simulation.repr_funcs import *\n",
    "from hardware_accelerators.simulation import SystolicArraySimulator\n",
    "from hardware_accelerators.nn import load_model, softmax\n",
    "from pyrtl import *\n",
    "import pyrtl\n",
    "import numpy as np\n",
    "from typing import Type\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: convert images to tensor and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "# Download MNIST test data\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    batch, labels = next(iter(loader))\n",
    "    return batch.reshape(batch_size, -1).numpy(), labels.numpy()\n",
    "\n",
    "\n",
    "def get_activation():\n",
    "    image, _ = next(iter(test_loader))\n",
    "    image = image.detach().numpy().reshape(-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyRTL lmul fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmul_fix(\n",
    "    float_a: WireVector, float_b: WireVector, dtype: Type[BaseFloat], fast=False\n",
    "):\n",
    "    e_bits, m_bits = dtype.exponent_bits(), dtype.mantissa_bits()\n",
    "    em_bits = e_bits + m_bits\n",
    "    sign_a = float_a[em_bits]\n",
    "    sign_b = float_b[em_bits]\n",
    "    exp_a = float_a[m_bits:-1]\n",
    "    exp_b = float_b[m_bits:-1]\n",
    "    exp_mantissa_a = float_a[:em_bits]\n",
    "    exp_mantissa_b = float_b[:em_bits]\n",
    "\n",
    "    zero_or_subnormal = WireVector(1)\n",
    "    carry_msb = WireVector(2)\n",
    "    fp_out = WireVector(dtype.bitwidth())\n",
    "\n",
    "    OFFSET_MINUS_BIAS = lmul_offset_rtl(dtype)\n",
    "    MAX_VALUE = pyrtl.Const(dtype.binary_max(), bitwidth=em_bits)\n",
    "\n",
    "    if fast:\n",
    "        final_sum = carrysave_adder(\n",
    "            exp_mantissa_a, exp_mantissa_b, OFFSET_MINUS_BIAS, final_adder=kogge_stone\n",
    "        )\n",
    "    else:\n",
    "        final_sum = exp_mantissa_a + exp_mantissa_b + OFFSET_MINUS_BIAS\n",
    "\n",
    "    carry_msb <<= final_sum[em_bits:]\n",
    "    zero_or_subnormal <<= ~pyrtl.or_all_bits(exp_a) | ~pyrtl.or_all_bits(exp_b)\n",
    "\n",
    "    with conditional_assignment:\n",
    "        with zero_or_subnormal:\n",
    "            fp_out |= 0\n",
    "        with carry_msb == 0:\n",
    "            fp_out |= 0\n",
    "        with carry_msb == 1:\n",
    "            fp_out |= pyrtl.concat(sign_a ^ sign_b, final_sum[:em_bits])\n",
    "        with pyrtl.otherwise:\n",
    "            fp_out |= pyrtl.concat(sign_a ^ sign_b, MAX_VALUE)\n",
    "\n",
    "    return fp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class FastLmul:\n",
    "    def __init__(self, dtype: Type[BaseFloat]):\n",
    "        self.dtype = dtype\n",
    "        bits = dtype.bitwidth()\n",
    "        reset_working_block()\n",
    "        input_a, input_b = Input(bits, \"input_a\"), Input(bits, \"input_b\")\n",
    "        self.output = Output(bits, \"output\")\n",
    "        self.output <<= lmul_fix(input_a, input_b, dtype)\n",
    "        self.sim = pyrtl.CompiledSimulation()\n",
    "        self.lmul_offset = {\n",
    "            # BF16: 16248,\n",
    "            BF16: get_combined_offset(8, 7, True),\n",
    "            # Float32: 1064828928,\n",
    "            Float32: get_combined_offset(8, 23, True),\n",
    "        }\n",
    "        self.bitmask = {\n",
    "            BF16: 0b0111111111111111,\n",
    "            Float32: 0b01111111111111111111111111111111,\n",
    "        }\n",
    "\n",
    "    def run(self, a: float, b: float) -> float:\n",
    "        self.sim.step(\n",
    "            {\"input_a\": self.dtype(a).binint, \"input_b\": self.dtype(b).binint},\n",
    "        )\n",
    "        return float(self.dtype(binint=self.sim.inspect(\"output\")))\n",
    "\n",
    "    def __call__(self, a: float, b: float) -> Any:\n",
    "        bin_a, bin_b = self.dtype(a).binint, self.dtype(b).binint\n",
    "        sign = (bin_a >> (self.dtype.bitwidth() - 1)) ^ (\n",
    "            bin_b >> (self.dtype.bitwidth() - 1)\n",
    "        )\n",
    "        bin_a &= self.bitmask[self.dtype]\n",
    "        bin_b &= self.bitmask[self.dtype]\n",
    "        if bin_a >> self.dtype.mantissa_bits() == 0:\n",
    "            return 0\n",
    "        if bin_b >> self.dtype.mantissa_bits() == 0:\n",
    "            return 0\n",
    "        binint = (bin_a + bin_b + self.lmul_offset[self.dtype]) & self.bitmask[\n",
    "            self.dtype\n",
    "        ]\n",
    "        binint |= sign << (self.dtype.bitwidth() - 1)\n",
    "        return float(self.dtype(binint=binint))\n",
    "\n",
    "\n",
    "def run_lmul(a: float, b: float, dtype: Type[BaseFloat] = BF16, fn=lmul_fix):\n",
    "    bits = dtype.bitwidth()\n",
    "\n",
    "    reset_working_block()\n",
    "    input_a, input_b = Input(bits, \"input_a\"), Input(bits, \"input_b\")\n",
    "    output = Output(bits, \"output\")\n",
    "    output <<= fn(input_a, input_b, dtype)\n",
    "\n",
    "    tracer = SimulationTrace(\"all\")\n",
    "    sim = Simulation(tracer=tracer)\n",
    "    sim.step({input_a: dtype(a).binint, input_b: dtype(b).binint})\n",
    "\n",
    "    binary_result = sim.inspect(output)\n",
    "    result = dtype(binint=binary_result)\n",
    "\n",
    "    print(f\"Expected: {float(dtype(a * b))}\")\n",
    "    print(f\"{a} + {b} = {result}\")\n",
    "    print(format(binary_result, f\"0{bits}b\"))\n",
    "\n",
    "    # if fn == lmul_fix:\n",
    "    #     carry_msb = sim.inspect(\"carry_msb\")\n",
    "    #     zero_flag = sim.inspect('zero_flag')\n",
    "    #     print(f\"carry_msb: {carry_msb}\")\n",
    "    #     print(f\"zero_flag: {zero_flag}\")\n",
    "\n",
    "    return float(result)\n",
    "\n",
    "\n",
    "def run_adder(\n",
    "    a: float | int, b: float | int, dtype: Type[BaseFloat] = BF16, binary_mode=False\n",
    "):\n",
    "    bits = dtype.bitwidth()\n",
    "    reset_working_block()\n",
    "    input_a, input_b = Input(bits, \"input_a\"), Input(bits, \"input_b\")\n",
    "    output = Output(bits, \"output\")\n",
    "    output <<= float_adder(input_a, input_b, dtype)\n",
    "    sim = Simulation()\n",
    "    if binary_mode:\n",
    "        sim.step({input_a: a, input_b: b})\n",
    "    else:\n",
    "        sim.step({input_a: dtype(a).binint, input_b: dtype(b).binint})\n",
    "\n",
    "    binary_result = sim.inspect(output)\n",
    "    result = dtype(binint=binary_result)\n",
    "\n",
    "    print(f\"Expected: {float(dtype(a + b))}\")\n",
    "    print(f\"{float(a):.3f} + {float(b):.3f} = {float(result):.3f}\")\n",
    "    print(format(binary_result, f\"0{bits}b\"))\n",
    "\n",
    "    return float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: -0.2236328125\n",
      "-0.32 + 0.7 = -0.216796875\n",
      "1011111001011110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.216796875"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_lmul(-0.32, 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3185277104182501e+38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.89 * 3.38953138925e37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0.0\n",
      "0 + -0.063 = 0.0\n",
      "0000000000000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_lmul(0, -0.063, fn=lmul_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BF16(binary='1011110110010101', decimal=-0.0732, decimal_approx=-0.07275390625)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_0 = 0b1000000000000000\n",
    "pos_0 = 0b0\n",
    "adder_in = BF16(-0.0732)\n",
    "adder_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 32768.0\n",
      "32768.000 + 0.000 = 0.000\n",
      "0000000000000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_adder(neg_0, 0, binary_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmulcls = FastLmul(BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 213 µs, sys: 16 µs, total: 229 µs\n",
      "Wall time: 222 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lmulcls.run(0, 0.000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized numpy lmul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Type, Tuple, Any\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16, Float32\n",
    "\n",
    "\n",
    "class OptimizedLmul:\n",
    "    \"\"\"Optimized implementation of LMUL using pure integer arithmetic\"\"\"\n",
    "\n",
    "    def __init__(self, dtype: Type[BaseFloat]):\n",
    "        self.dtype = dtype\n",
    "        self.lmul_offset = {\n",
    "            BF16: get_combined_offset(8, 7, True),\n",
    "            Float32: get_combined_offset(8, 23, True),\n",
    "        }\n",
    "        self.bitmask = {\n",
    "            BF16: 0b0111111111111111,\n",
    "            Float32: 0b01111111111111111111111111111111,\n",
    "        }\n",
    "        self.bitwidth = dtype.bitwidth()\n",
    "        self.mantissa_bits = dtype.mantissa_bits()\n",
    "\n",
    "    def multiply(self, bin_a: int, bin_b: int) -> int:\n",
    "        \"\"\"Multiply two numbers in binint representation using LMUL algorithm\"\"\"\n",
    "        # Extract sign bit\n",
    "        sign = (bin_a >> (self.bitwidth - 1)) ^ (bin_b >> (self.bitwidth - 1))\n",
    "\n",
    "        # Clear sign bits\n",
    "        bin_a &= self.bitmask[self.dtype]\n",
    "        bin_b &= self.bitmask[self.dtype]\n",
    "\n",
    "        # Check for zero exponents (denormals or zero)\n",
    "        if bin_a >> self.mantissa_bits == 0 or bin_b >> self.mantissa_bits == 0:\n",
    "            return 0\n",
    "\n",
    "        # Apply LMUL algorithm\n",
    "        binint = (bin_a + bin_b + self.lmul_offset[self.dtype]) & self.bitmask[\n",
    "            self.dtype\n",
    "        ]\n",
    "\n",
    "        # Set sign bit\n",
    "        binint |= sign << (self.bitwidth - 1)\n",
    "\n",
    "        return binint\n",
    "\n",
    "\n",
    "def convert_to_binint(data: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"Convert numpy array of floats to array of binint representations\"\"\"\n",
    "    # Create a vectorized function to convert each element\n",
    "    vectorized_convert = np.vectorize(lambda x: dtype(x).binint)\n",
    "    return vectorized_convert(data)\n",
    "\n",
    "\n",
    "def convert_from_binint(data: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"Convert numpy array of binint representations back to floats\"\"\"\n",
    "    # Create a vectorized function to convert each element\n",
    "    vectorized_convert = np.vectorize(lambda x: float(dtype(binint=x)))\n",
    "    return vectorized_convert(data)\n",
    "\n",
    "\n",
    "def relu_binint(x: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"Apply ReLU to binint values by checking sign bit\"\"\"\n",
    "    # For floating point in binint representation, negative numbers have the highest bit set\n",
    "    sign_mask = 1 << (dtype.bitwidth() - 1)\n",
    "    return np.where((x & sign_mask) == 0, x, 0)\n",
    "\n",
    "\n",
    "def optimized_mlp_inference(\n",
    "    inputs_batch: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Optimized MLP inference using integer arithmetic for batched inputs\n",
    "\n",
    "    Args:\n",
    "        inputs_batch: Batch of input vectors [batch_size, input_dim]\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predicted_classes, output_probabilities)\n",
    "    \"\"\"\n",
    "    # Initialize the optimized LMUL\n",
    "    lmul = OptimizedLmul(dtype)\n",
    "\n",
    "    # Pre-convert all weights and biases to binint representation\n",
    "    fc1_weight_bin = convert_to_binint(model_weights[\"fc1_weight\"], dtype)\n",
    "    fc1_bias_bin = convert_to_binint(model_weights[\"fc1_bias\"], dtype)\n",
    "    fc2_weight_bin = convert_to_binint(model_weights[\"fc2_weight\"], dtype)\n",
    "    fc2_bias_bin = convert_to_binint(model_weights[\"fc2_bias\"], dtype)\n",
    "\n",
    "    # Convert inputs to binint\n",
    "    inputs_bin = convert_to_binint(inputs_batch, dtype)\n",
    "\n",
    "    batch_size = inputs_batch.shape[0]\n",
    "    hidden_size = fc1_weight_bin.shape[0]\n",
    "    output_size = fc2_weight_bin.shape[0]\n",
    "    input_size = inputs_batch.shape[1]\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1_bin = np.zeros((batch_size, hidden_size), dtype=np.int64)\n",
    "\n",
    "    # Perform matrix multiplication with LMUL\n",
    "    for b in range(batch_size):\n",
    "        for i in range(hidden_size):\n",
    "            acc = 0  # Accumulate in regular float for now\n",
    "            for j in range(input_size):\n",
    "                # Multiply using optimized LMUL\n",
    "                prod = lmul.multiply(fc1_weight_bin[i, j], inputs_bin[b, j])\n",
    "\n",
    "                # Convert product back to float for accumulation\n",
    "                # In a real hardware implementation, this would be a floating-point addition\n",
    "                prod_float = float(dtype(binint=prod))\n",
    "                acc += prod_float\n",
    "\n",
    "            # Convert accumulated result back to binint\n",
    "            h1_bin[b, i] = dtype(acc).binint\n",
    "\n",
    "    # Add bias (in binint space, this is still a floating-point addition)\n",
    "    h1_with_bias_bin = np.zeros_like(h1_bin)\n",
    "    for b in range(batch_size):\n",
    "        for i in range(hidden_size):\n",
    "            # Convert to float, add, then convert back to binint\n",
    "            h1_float = float(dtype(binint=h1_bin[b, i]))\n",
    "            bias_float = float(dtype(binint=fc1_bias_bin[i]))\n",
    "            h1_with_bias_bin[b, i] = dtype(h1_float + bias_float).binint\n",
    "\n",
    "    # Apply ReLU in binint space\n",
    "    a1_bin = relu_binint(h1_with_bias_bin, dtype)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias\n",
    "    h_out_bin = np.zeros((batch_size, output_size), dtype=np.int64)\n",
    "\n",
    "    # Perform matrix multiplication with LMUL\n",
    "    for b in range(batch_size):\n",
    "        for i in range(output_size):\n",
    "            acc = 0  # Accumulate in regular float for now\n",
    "            for j in range(hidden_size):\n",
    "                # Multiply using optimized LMUL\n",
    "                prod = lmul.multiply(fc2_weight_bin[i, j], a1_bin[b, j])\n",
    "\n",
    "                # Convert product back to float for accumulation\n",
    "                prod_float = float(dtype(binint=prod))\n",
    "                acc += prod_float\n",
    "\n",
    "            # Convert accumulated result back to binint\n",
    "            h_out_bin[b, i] = dtype(acc).binint\n",
    "\n",
    "    # Add bias\n",
    "    h_out_with_bias_bin = np.zeros_like(h_out_bin)\n",
    "    for b in range(batch_size):\n",
    "        for i in range(output_size):\n",
    "            # Convert to float, add, then convert back to binint\n",
    "            h_out_float = float(dtype(binint=h_out_bin[b, i]))\n",
    "            bias_float = float(dtype(binint=fc2_bias_bin[i]))\n",
    "            h_out_with_bias_bin[b, i] = dtype(h_out_float + bias_float).binint\n",
    "\n",
    "    # Convert final layer output back to floats for softmax\n",
    "    h_out_float = convert_from_binint(h_out_with_bias_bin, dtype)\n",
    "\n",
    "    # Apply softmax (in float space)\n",
    "    output_probs = np.zeros_like(h_out_float)\n",
    "    for b in range(batch_size):\n",
    "        exp_x = np.exp(\n",
    "            h_out_float[b] - np.max(h_out_float[b])\n",
    "        )  # For numerical stability\n",
    "        output_probs[b] = exp_x / exp_x.sum()\n",
    "\n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(output_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pyinstrument extension is already loaded. To reload it, use:\n",
      "  %reload_ext pyinstrument\n"
     ]
    }
   ],
   "source": [
    "%load_ext pyinstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11158.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NumPy inference time for batch of 10: 0.5987 seconds\n",
      "Optimized inference time for batch of 10: 7.7360 seconds\n",
      "Speedup: 0.08x\n",
      "Prediction match rate: 10/10 (100.00%)\n",
      "\n",
      "Sample comparisons:\n",
      "Sample 0: NumPy predicted 7, Optimized predicted 7\n",
      "Sample 1: NumPy predicted 2, Optimized predicted 2\n",
      "Sample 2: NumPy predicted 1, Optimized predicted 1\n",
      "Sample 3: NumPy predicted 0, Optimized predicted 0\n",
      "Sample 4: NumPy predicted 4, Optimized predicted 4\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "    import time\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"../models/mlp_mnist_bf16.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get batch of input data (assuming get_batch_activations() returns multiple samples)\n",
    "    batch_size = 10\n",
    "    inputs_batch, labels = get_batch(batch_size)\n",
    "\n",
    "    # Run standard NumPy inference for comparison\n",
    "    start_time = time.time()\n",
    "\n",
    "    numpy_results = []\n",
    "    for i in tqdm(range(batch_size)):\n",
    "        inputs = inputs_batch[i]\n",
    "        h1_numpy = inputs @ fc1_weight.T + fc1_bias\n",
    "        a1_numpy = np.maximum(0, h1_numpy)\n",
    "        h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "        a_out_numpy = softmax(h_out_numpy)\n",
    "        predicted_class_numpy = np.argmax(a_out_numpy)\n",
    "        numpy_results.append((predicted_class_numpy, a_out_numpy))\n",
    "\n",
    "    numpy_time = time.time() - start_time\n",
    "    numpy_predictions = np.array([r[0] for r in numpy_results])\n",
    "\n",
    "    print(\n",
    "        f\"Standard NumPy inference time for batch of {batch_size}: {numpy_time:.4f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Run optimized inference using integer arithmetic\n",
    "    start_time = time.time()\n",
    "    predicted_classes, output_probs = optimized_mlp_inference(\n",
    "        inputs_batch, model_weights, BF16\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Optimized inference time for batch of {batch_size}: {optimized_time:.4f} seconds\"\n",
    "    )\n",
    "    print(f\"Speedup: {numpy_time / optimized_time:.2f}x\")\n",
    "\n",
    "    # Compare results\n",
    "    match_count = np.sum(predicted_classes == numpy_predictions)\n",
    "    print(\n",
    "        f\"Prediction match rate: {match_count}/{batch_size} ({match_count/batch_size*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Show a few examples\n",
    "    print(\"\\nSample comparisons:\")\n",
    "    for i in range(min(5, batch_size)):\n",
    "        print(\n",
    "            f\"Sample {i}: NumPy predicted {numpy_predictions[i]}, Optimized predicted {predicted_classes[i]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NumPy inference time for batch of 128: 0.0949 seconds\n",
      "Optimized vectorized inference time for batch of 128: 1.5121 seconds\n",
      "Speedup: 0.06x\n",
      "Prediction match rate: 3/128 (2.34%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from typing import Type, Tuple, Dict, Any\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16, Float32\n",
    "\n",
    "\n",
    "def float_to_binint_batch(values: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized conversion of float values to binint representation\n",
    "\n",
    "    Args:\n",
    "        values: NumPy array of float values\n",
    "        dtype: Target floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # For BF16, we need to convert through float32 first\n",
    "        # Get binary representation of float32 values\n",
    "        float32_bits = np.frombuffer(\n",
    "            struct.pack(\"!%df\" % len(values.flatten()), *values.flatten()),\n",
    "            dtype=np.uint32,\n",
    "        ).reshape(values.shape)\n",
    "\n",
    "        # Extract parts from float32 and construct BF16\n",
    "        sign = (float32_bits >> 31) & 0x1\n",
    "        exp = (float32_bits >> 23) & 0xFF\n",
    "        mantissa = (float32_bits >> 16) & 0x7F  # Keep only top 7 bits of mantissa\n",
    "\n",
    "        # Combine into BF16 binint (16 bits)\n",
    "        return ((sign << 15) | (exp << 7) | mantissa).astype(np.uint16)\n",
    "\n",
    "    elif dtype == Float32:\n",
    "        # For Float32, we can directly convert\n",
    "        return np.frombuffer(\n",
    "            struct.pack(\"!%df\" % len(values.flatten()), *values.flatten()),\n",
    "            dtype=np.uint32,\n",
    "        ).reshape(values.shape)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "def binint_to_float_batch(binints: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized conversion of binint representations back to float values\n",
    "\n",
    "    Args:\n",
    "        binints: NumPy array of binint representations\n",
    "        dtype: Source floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of float values\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # Extract parts from BF16\n",
    "        sign = (binints >> 15) & 0x1\n",
    "        exp = (binints >> 7) & 0xFF\n",
    "        mantissa = binints & 0x7F\n",
    "\n",
    "        # Construct float32 representation\n",
    "        float32_bits = (sign << 31) | (exp << 23) | (mantissa << 16)\n",
    "\n",
    "        # Convert to float32\n",
    "        return np.frombuffer(\n",
    "            struct.pack(\"!%dI\" % len(float32_bits.flatten()), *float32_bits.flatten()),\n",
    "            dtype=np.float32,\n",
    "        ).reshape(binints.shape)\n",
    "\n",
    "    elif dtype == Float32:\n",
    "        # For Float32, we can directly convert\n",
    "        return np.frombuffer(\n",
    "            struct.pack(\"!%dI\" % len(binints.flatten()), *binints.flatten()),\n",
    "            dtype=np.float32,\n",
    "        ).reshape(binints.shape)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "\n",
    "def lmul_vectorized(\n",
    "    a_binint: np.ndarray, b_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized implementation of LMUL algorithm\n",
    "\n",
    "    Args:\n",
    "        a_binint: NumPy array of binint representations\n",
    "        b_binint: NumPy array of binint representations\n",
    "        dtype: Floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations of results\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # Constants for BF16\n",
    "        bitmask = 0x7FFF  # 15 bits (excluding sign)\n",
    "        bitwidth = 16\n",
    "        mantissa_bits = 7\n",
    "        lmul_offset = get_combined_offset(8, 7, True)\n",
    "    elif dtype == Float32:\n",
    "        # Constants for Float32\n",
    "        bitmask = 0x7FFFFFFF  # 31 bits (excluding sign)\n",
    "        bitwidth = 32\n",
    "        mantissa_bits = 23\n",
    "        lmul_offset = get_combined_offset(8, 23, True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "    # Extract sign bits\n",
    "    sign_a = (a_binint >> (bitwidth - 1)) & 0x1\n",
    "    sign_b = (b_binint >> (bitwidth - 1)) & 0x1\n",
    "\n",
    "    # Compute result sign\n",
    "    sign_result = sign_a ^ sign_b\n",
    "\n",
    "    # Clear sign bits\n",
    "    a_unsigned = a_binint & bitmask\n",
    "    b_unsigned = b_binint & bitmask\n",
    "\n",
    "    # Create masks for zero exponents\n",
    "    a_exp = a_unsigned >> mantissa_bits\n",
    "    b_exp = b_unsigned >> mantissa_bits\n",
    "\n",
    "    # Apply LMUL algorithm where both exponents are non-zero\n",
    "    result = np.zeros_like(a_binint)\n",
    "    valid_mask = (a_exp != 0) & (b_exp != 0)\n",
    "\n",
    "    if np.any(valid_mask):\n",
    "        # Only compute for valid inputs\n",
    "        result[valid_mask] = (\n",
    "            a_unsigned[valid_mask] + b_unsigned[valid_mask] + lmul_offset\n",
    "        ) & bitmask\n",
    "\n",
    "        # Set sign bits\n",
    "        result[valid_mask] |= sign_result[valid_mask] << (bitwidth - 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def matrix_vector_multiply_lmul(\n",
    "    weights_binint: np.ndarray, inputs_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Optimized matrix-vector multiplication using vectorized LMUL\n",
    "\n",
    "    Args:\n",
    "        weights_binint: Weight matrix in binint representation [output_dim, input_dim]\n",
    "        inputs_binint: Input vector in binint representation [batch_size, input_dim]\n",
    "        dtype: Floating-point type\n",
    "\n",
    "    Returns:\n",
    "        Result in float representation [batch_size, output_dim]\n",
    "    \"\"\"\n",
    "    batch_size, input_dim = inputs_binint.shape\n",
    "    output_dim = weights_binint.shape[0]\n",
    "\n",
    "    # Reshape inputs for broadcasting\n",
    "    inputs_reshaped = inputs_binint.reshape(batch_size, 1, input_dim)\n",
    "\n",
    "    # Broadcast weights for batch processing\n",
    "    weights_broadcast = np.broadcast_to(\n",
    "        weights_binint.reshape(1, output_dim, input_dim),\n",
    "        (batch_size, output_dim, input_dim),\n",
    "    )\n",
    "\n",
    "    # Apply LMUL to all pairs of weights and inputs\n",
    "    products_binint = lmul_vectorized(\n",
    "        weights_broadcast.reshape(-1),\n",
    "        np.broadcast_to(inputs_reshaped, (batch_size, output_dim, input_dim)).reshape(\n",
    "            -1\n",
    "        ),\n",
    "        dtype,\n",
    "    ).reshape(batch_size, output_dim, input_dim)\n",
    "\n",
    "    # Convert products to float for summation\n",
    "    products_float = binint_to_float_batch(products_binint, dtype)\n",
    "\n",
    "    # Sum along input dimension\n",
    "    result_float = np.sum(products_float, axis=2)\n",
    "\n",
    "    return result_float\n",
    "\n",
    "\n",
    "def add_bias_vectorized(activations: np.ndarray, bias: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Add bias to activations (in float space)\"\"\"\n",
    "    return activations + bias\n",
    "\n",
    "\n",
    "def relu_vectorized(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply ReLU activation\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax_vectorized(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply softmax activation\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def optimized_mlp_inference_vectorized(\n",
    "    inputs_batch: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Highly optimized MLP inference using vectorized operations\n",
    "\n",
    "    Args:\n",
    "        inputs_batch: Batch of input vectors [batch_size, input_dim]\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predicted_classes, output_probabilities)\n",
    "    \"\"\"\n",
    "    # Pre-convert all weights and biases to binint representation\n",
    "    fc1_weight_bin = float_to_binint_batch(model_weights[\"fc1_weight\"], dtype)\n",
    "    fc1_bias = model_weights[\"fc1_bias\"]  # Keep biases as float for addition\n",
    "    fc2_weight_bin = float_to_binint_batch(model_weights[\"fc2_weight\"], dtype)\n",
    "    fc2_bias = model_weights[\"fc2_bias\"]  # Keep biases as float for addition\n",
    "\n",
    "    # Convert inputs to binint\n",
    "    inputs_bin = float_to_binint_batch(inputs_batch, dtype)\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1_float = matrix_vector_multiply_lmul(fc1_weight_bin, inputs_bin, dtype)\n",
    "    h1_with_bias = add_bias_vectorized(h1_float, fc1_bias)\n",
    "    a1 = relu_vectorized(h1_with_bias)\n",
    "\n",
    "    # Convert activations back to binint for next layer\n",
    "    a1_bin = float_to_binint_batch(a1, dtype)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias + softmax\n",
    "    h_out_float = matrix_vector_multiply_lmul(fc2_weight_bin, a1_bin, dtype)\n",
    "    h_out_with_bias = add_bias_vectorized(h_out_float, fc2_bias)\n",
    "\n",
    "    # Apply softmax\n",
    "    output_probs = softmax_vectorized(h_out_with_bias)\n",
    "\n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(output_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, output_probs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "    import time\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"../models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get batch of input data\n",
    "    batch_size = 128\n",
    "    inputs_batch, labels = get_batch(batch_size)\n",
    "\n",
    "    # Run standard NumPy inference for comparison\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Vectorized standard NumPy implementation\n",
    "    h1_numpy = inputs_batch @ fc1_weight.T + fc1_bias\n",
    "    a1_numpy = np.maximum(0, h1_numpy)\n",
    "    h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "    exp_x = np.exp(h_out_numpy - np.max(h_out_numpy, axis=1, keepdims=True))\n",
    "    a_out_numpy = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    numpy_predictions = np.argmax(a_out_numpy, axis=1)\n",
    "\n",
    "    numpy_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"Standard NumPy inference time for batch of {batch_size}: {numpy_time:.4f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Run optimized inference using vectorized operations\n",
    "    start_time = time.time()\n",
    "    predicted_classes, output_probs = optimized_mlp_inference_vectorized(\n",
    "        inputs_batch, model_weights, BF16\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Optimized vectorized inference time for batch of {batch_size}: {optimized_time:.4f} seconds\"\n",
    "    )\n",
    "    print(f\"Speedup: {numpy_time / optimized_time:.2f}x\")\n",
    "\n",
    "    # Compare results\n",
    "    match_count = np.sum(predicted_classes == numpy_predictions)\n",
    "    print(\n",
    "        f\"Prediction match rate: {match_count}/{batch_size} ({match_count/batch_size*100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0625, 4.25, 8.5]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hardware_accelerators.simulation.matrix_utils import convert_array_dtype\n",
    "\n",
    "bin_array = convert_array_dtype(np.array([0, 1, 2, 3]), BF16)\n",
    "\n",
    "[BF16(binint=x).decimal_approx for x in lmul_vectorized(bin_array, bin_array, BF16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_batch shape: (8, 784) <class 'numpy.ndarray'>\n",
      "fc1_weight shape: (128, 784)\n",
      "fc1_bias shape: (128,)\n",
      "inputs_batch dtype: float32\n",
      "Is inputs_batch object dtype? False\n",
      "Standard NumPy inference time for batch of 8: 0.7960 seconds\n",
      "Optimized inference time for batch of 8: 5.6023 seconds\n",
      "Speedup: 0.14x\n",
      "Prediction match rate: 0/8 (0.00%)\n",
      "\n",
      "Detailed comparison for first 5 examples:\n",
      "Example 0:\n",
      "  NumPy prediction: 7\n",
      "  Optimized prediction: 8\n",
      "  Top NumPy probabilities: [3.2747920e-07 4.0034397e-06 9.9999571e-01]\n",
      "  Top Optimized probabilities: [0.10660665 0.11183428 0.12050679]\n",
      "Example 1:\n",
      "  NumPy prediction: 2\n",
      "  Optimized prediction: 8\n",
      "  Top NumPy probabilities: [4.4528181e-07 1.6522257e-05 9.9998283e-01]\n",
      "  Top Optimized probabilities: [0.10660665 0.11183428 0.12050679]\n",
      "Example 2:\n",
      "  NumPy prediction: 1\n",
      "  Optimized prediction: 8\n",
      "  Top NumPy probabilities: [1.5195979e-04 2.6030971e-03 9.9687707e-01]\n",
      "  Top Optimized probabilities: [0.10660665 0.11183428 0.12050679]\n",
      "Example 3:\n",
      "  NumPy prediction: 0\n",
      "  Optimized prediction: 8\n",
      "  Top NumPy probabilities: [1.9248056e-07 2.1894548e-06 9.9999762e-01]\n",
      "  Top Optimized probabilities: [0.10660665 0.11183428 0.12050679]\n",
      "Example 4:\n",
      "  NumPy prediction: 4\n",
      "  Optimized prediction: 8\n",
      "  Top NumPy probabilities: [3.5149398e-07 4.1032373e-04 9.9958938e-01]\n",
      "  Top Optimized probabilities: [0.10660665 0.11183428 0.12050679]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Type, Tuple, Dict, Any\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16, Float32\n",
    "\n",
    "\n",
    "def convert_array_dtype(values: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert array of float values to array of binint representations using the dtype class\n",
    "\n",
    "    Args:\n",
    "        values: NumPy array of float values\n",
    "        dtype: Target floating-point type\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations\n",
    "    \"\"\"\n",
    "    # Flatten the array for processing\n",
    "    original_shape = values.shape\n",
    "    flat_values = values.flatten()\n",
    "\n",
    "    # Create array to hold results\n",
    "    result = np.zeros(\n",
    "        flat_values.shape, dtype=np.uint32 if dtype.bitwidth() == 32 else np.uint16\n",
    "    )\n",
    "\n",
    "    # Convert each value using the dtype class\n",
    "    for i, val in enumerate(flat_values):\n",
    "        result[i] = dtype(val).binint\n",
    "\n",
    "    # Reshape back to original shape\n",
    "    return result.reshape(original_shape)\n",
    "\n",
    "\n",
    "def convert_binint_to_float(binints: np.ndarray, dtype: Type[BaseFloat]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert array of binint representations back to float values using the dtype class\n",
    "\n",
    "    Args:\n",
    "        binints: NumPy array of binint representations\n",
    "        dtype: Source floating-point type\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of float values\n",
    "    \"\"\"\n",
    "    # Flatten the array for processing\n",
    "    original_shape = binints.shape\n",
    "    flat_binints = binints.flatten()\n",
    "\n",
    "    # Create array to hold results\n",
    "    result = np.zeros(flat_binints.shape, dtype=np.float32)\n",
    "\n",
    "    # Convert each binint using the dtype class\n",
    "    for i, binint in enumerate(flat_binints):\n",
    "        result[i] = float(dtype(binint=int(binint)))\n",
    "\n",
    "    # Reshape back to original shape\n",
    "    return result.reshape(original_shape)\n",
    "\n",
    "\n",
    "def lmul_vectorized(\n",
    "    a_binint: np.ndarray, b_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized implementation of LMUL algorithm\n",
    "\n",
    "    Args:\n",
    "        a_binint: NumPy array of binint representations\n",
    "        b_binint: NumPy array of binint representations\n",
    "        dtype: Floating-point type (BF16 or Float32)\n",
    "\n",
    "    Returns:\n",
    "        NumPy array of binint representations of results\n",
    "    \"\"\"\n",
    "    if dtype == BF16:\n",
    "        # Constants for BF16\n",
    "        bitmask = 0x7FFF  # 15 bits (excluding sign)\n",
    "        bitwidth = 16\n",
    "        mantissa_bits = 7\n",
    "        lmul_offset = get_combined_offset(8, 7, True)\n",
    "    elif dtype == Float32:\n",
    "        # Constants for Float32\n",
    "        bitmask = 0x7FFFFFFF  # 31 bits (excluding sign)\n",
    "        bitwidth = 32\n",
    "        mantissa_bits = 23\n",
    "        lmul_offset = get_combined_offset(8, 23, True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "    # Create output array\n",
    "    result = np.zeros_like(a_binint)\n",
    "\n",
    "    # Flatten arrays for processing\n",
    "    a_flat = a_binint.flatten()\n",
    "    b_flat = b_binint.flatten()\n",
    "    result_flat = result.flatten()\n",
    "\n",
    "    # Process each pair of values\n",
    "    for i in range(len(a_flat)):\n",
    "        a_val = int(a_flat[i])\n",
    "        b_val = int(b_flat[i])\n",
    "\n",
    "        # Extract sign bits\n",
    "        sign_a = (a_val >> (bitwidth - 1)) & 0x1\n",
    "        sign_b = (b_val >> (bitwidth - 1)) & 0x1\n",
    "        sign_result = sign_a ^ sign_b\n",
    "\n",
    "        # Clear sign bits\n",
    "        a_unsigned = a_val & bitmask\n",
    "        b_unsigned = b_val & bitmask\n",
    "\n",
    "        # Check for zero exponents\n",
    "        a_exp = a_unsigned >> mantissa_bits\n",
    "        b_exp = b_unsigned >> mantissa_bits\n",
    "\n",
    "        if a_exp == 0 or b_exp == 0:\n",
    "            result_flat[i] = 0\n",
    "        else:\n",
    "            # Apply LMUL algorithm\n",
    "            result_val = (a_unsigned + b_unsigned + lmul_offset) & bitmask\n",
    "            result_val |= sign_result << (bitwidth - 1)\n",
    "            result_flat[i] = result_val\n",
    "\n",
    "    return result.reshape(a_binint.shape)\n",
    "\n",
    "\n",
    "def batch_matrix_vector_multiply(\n",
    "    weights_binint: np.ndarray, inputs_binint: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform matrix-vector multiplication for a batch of inputs using LMUL\n",
    "\n",
    "    Args:\n",
    "        weights_binint: Weight matrix in binint representation [output_dim, input_dim]\n",
    "        inputs_binint: Input vectors in binint representation [batch_size, input_dim]\n",
    "        dtype: Floating-point type\n",
    "\n",
    "    Returns:\n",
    "        Result in float representation [batch_size, output_dim]\n",
    "    \"\"\"\n",
    "    batch_size, input_dim = inputs_binint.shape\n",
    "    output_dim, input_dim_w = weights_binint.shape\n",
    "\n",
    "    if input_dim != input_dim_w:\n",
    "        raise ValueError(\n",
    "            f\"Dimension mismatch: inputs {input_dim} vs weights {input_dim_w}\"\n",
    "        )\n",
    "\n",
    "    # Initialize result array\n",
    "    result_float = np.zeros((batch_size, output_dim), dtype=np.float32)\n",
    "\n",
    "    # Process each batch item and output neuron\n",
    "    for b in range(batch_size):\n",
    "        for o in range(output_dim):\n",
    "            # Multiply weights with inputs using LMUL\n",
    "            products_binint = lmul_vectorized(\n",
    "                np.broadcast_to(weights_binint[o], input_dim), inputs_binint[b], dtype\n",
    "            )\n",
    "\n",
    "            # Convert products to float for summation\n",
    "            products_float = convert_binint_to_float(products_binint, dtype)\n",
    "\n",
    "            # Sum the products\n",
    "            result_float[b, o] = np.sum(products_float)\n",
    "\n",
    "    return result_float\n",
    "\n",
    "\n",
    "def optimized_mlp_inference(\n",
    "    inputs_batch: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Optimized MLP inference using LMUL for matrix multiplications\n",
    "\n",
    "    Args:\n",
    "        inputs_batch: Batch of input vectors [batch_size, input_dim]\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (predicted_classes, output_probabilities)\n",
    "    \"\"\"\n",
    "    # Pre-convert all weights to binint representation\n",
    "    fc1_weight_bin = convert_array_dtype(model_weights[\"fc1_weight\"], dtype)\n",
    "    fc1_bias = model_weights[\"fc1_bias\"]  # Keep biases as float for addition\n",
    "    fc2_weight_bin = convert_array_dtype(model_weights[\"fc2_weight\"], dtype)\n",
    "    fc2_bias = model_weights[\"fc2_bias\"]  # Keep biases as float for addition\n",
    "\n",
    "    # Convert inputs to binint\n",
    "    inputs_bin = convert_array_dtype(inputs_batch, dtype)\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1_float = batch_matrix_vector_multiply(fc1_weight_bin, inputs_bin, dtype)\n",
    "    h1_with_bias = h1_float + fc1_bias  # Add bias\n",
    "    a1 = np.maximum(0, h1_with_bias)  # ReLU\n",
    "\n",
    "    # Convert activations back to binint for next layer\n",
    "    a1_bin = convert_array_dtype(a1, dtype)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias\n",
    "    h_out_float = batch_matrix_vector_multiply(fc2_weight_bin, a1_bin, dtype)\n",
    "    h_out_with_bias = h_out_float + fc2_bias  # Add bias\n",
    "\n",
    "    # Apply softmax\n",
    "    exp_x = np.exp(h_out_with_bias - np.max(h_out_with_bias, axis=1, keepdims=True))\n",
    "    output_probs = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Get predicted classes\n",
    "    predicted_classes = np.argmax(output_probs, axis=1)\n",
    "\n",
    "    return predicted_classes, output_probs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "    import time\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"../models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get batch of input data\n",
    "    batch_size = 8  # Smaller batch for testing\n",
    "    inputs_batch, _ = get_batch(batch_size)\n",
    "\n",
    "    # Run standard NumPy inference for comparison\n",
    "    start_time = time.time()\n",
    "\n",
    "    ###CHANGE: ENSURE THAT THE DIMENSIONS ARE CORRECT\n",
    "    print(\"inputs_batch shape:\", inputs_batch.shape, type(inputs_batch))\n",
    "    print(\"fc1_weight shape:\", fc1_weight.shape)\n",
    "    print(\"fc1_bias shape:\", fc1_bias.shape)\n",
    "    \n",
    "    # Also check if inputs_batch is a proper 2D numeric array\n",
    "    print(\"inputs_batch dtype:\", inputs_batch.dtype)\n",
    "    print(\"Is inputs_batch object dtype?\", inputs_batch.dtype == object)\n",
    "    ###\n",
    "    \n",
    "    # Vectorized standard NumPy implementation\n",
    "    h1_numpy = inputs_batch @ fc1_weight.T + fc1_bias\n",
    "    a1_numpy = np.maximum(0, h1_numpy)\n",
    "    h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "    exp_x = np.exp(h_out_numpy - np.max(h_out_numpy, axis=1, keepdims=True))\n",
    "    a_out_numpy = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    numpy_predictions = np.argmax(a_out_numpy, axis=1)\n",
    "\n",
    "    numpy_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"Standard NumPy inference time for batch of {batch_size}: {numpy_time:.4f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Run optimized inference\n",
    "    start_time = time.time()\n",
    "    predicted_classes, output_probs = optimized_mlp_inference(\n",
    "        inputs_batch, model_weights, BF16\n",
    "    )\n",
    "    optimized_time = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Optimized inference time for batch of {batch_size}: {optimized_time:.4f} seconds\"\n",
    "    )\n",
    "    print(f\"Speedup: {numpy_time / optimized_time:.2f}x\")\n",
    "\n",
    "    # Compare results\n",
    "    match_count = np.sum(predicted_classes == numpy_predictions)\n",
    "    print(\n",
    "        f\"Prediction match rate: {match_count}/{batch_size} ({match_count/batch_size*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Print detailed comparison for first few examples\n",
    "    print(\"\\nDetailed comparison for first 5 examples:\")\n",
    "    for i in range(min(5, batch_size)):\n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"  NumPy prediction: {numpy_predictions[i]}\")\n",
    "        print(f\"  Optimized prediction: {predicted_classes[i]}\")\n",
    "        print(f\"  Top NumPy probabilities: {np.sort(a_out_numpy[i])[-3:]}\")\n",
    "        print(f\"  Top Optimized probabilities: {np.sort(output_probs[i])[-3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 16:35:30  Samples:  326\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.034     CPU time: 0.035\n",
      "/   _/                      v5.0.1\n",
      "\n",
      "Profile at /tmp/ipykernel_1177/3627724169.py:4\n",
      "\n",
      "\u001b[31m0.033\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZMQInteractiveShell.run_ast_nodes\u001b[0m  \u001b[2mIPython/core/interactiveshell.py:3349\u001b[0m\n",
      "├─ \u001b[31m0.025\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<module>\u001b[0m  \u001b[2m/tmp/ipykernel_1177/3627724169.py:1\u001b[0m\n",
      "│  ├─ \u001b[31m0.023\u001b[0m \u001b[48;5;24m\u001b[38;5;15mFastLmul.__call__\u001b[0m  \u001b[2m/tmp/ipykernel_1177/493113130.py:30\u001b[0m\n",
      "│  │  ├─ \u001b[31m0.020\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16.__init__\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py:30\u001b[0m\n",
      "│  │  │  ├─ \u001b[31m0.020\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16._init_from_value\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py:102\u001b[0m\n",
      "│  │  │  │  ├─ \u001b[33m0.017\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16._update_all_representations\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py:144\u001b[0m\n",
      "│  │  │  │  │  ├─ \u001b[33m0.009\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16._binary_to_decimal\u001b[0m  \u001b[2mhardware_accelerators/dtypes/bfloat16.py:73\u001b[0m\n",
      "│  │  │  │  │  │  ├─ \u001b[32m0.004\u001b[0m [self]\u001b[0m  \u001b[2mhardware_accelerators/dtypes/bfloat16.py\u001b[0m\n",
      "│  │  │  │  │  │  ├─ \u001b[32m0.004\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<genexpr>\u001b[0m  \u001b[2mhardware_accelerators/dtypes/bfloat16.py:76\u001b[0m\n",
      "│  │  │  │  │  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16._bf16_parts_to_float32\u001b[0m  \u001b[2mhardware_accelerators/dtypes/bfloat16.py:48\u001b[0m\n",
      "│  │  │  │  │  ├─ \u001b[32m0.004\u001b[0m [self]\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py\u001b[0m\n",
      "│  │  │  │  │  └─ \u001b[32m0.004\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<genexpr>\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py:149\u001b[0m\n",
      "│  │  │  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m [self]\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py\u001b[0m\n",
      "│  │  │  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m BaseFloat.__instancecheck__\u001b[0m  \u001b[2m<frozen abc>:117\u001b[0m\n",
      "│  │  │  │  └─ \u001b[92m\u001b[2m0.000\u001b[0m isinstance\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "│  │  │  └─ \u001b[92m\u001b[2m0.000\u001b[0m [self]\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py\u001b[0m\n",
      "│  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16.bitwidth\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py:70\u001b[0m\n",
      "│  │  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16.format_spec\u001b[0m  \u001b[2mhardware_accelerators/dtypes/bfloat16.py:15\u001b[0m\n",
      "│  │  │  └─ \u001b[92m\u001b[2m0.000\u001b[0m [self]\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py\u001b[0m\n",
      "│  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m [self]\u001b[0m  \u001b[2m/tmp/ipykernel_1177/493113130.py\u001b[0m\n",
      "│  │  └─ \u001b[92m\u001b[2m0.000\u001b[0m \u001b[48;5;24m\u001b[38;5;15mBF16.mantissa_bits\u001b[0m  \u001b[2mhardware_accelerators/dtypes/base.py:78\u001b[0m\n",
      "│  ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mProfiler.stop\u001b[0m  \u001b[2mpyinstrument/profiler.py:168\u001b[0m\n",
      "│  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mStackSampler.unsubscribe\u001b[0m  \u001b[2mpyinstrument/stack_sampler.py:96\u001b[0m\n",
      "│  ├─ \u001b[92m\u001b[2m0.000\u001b[0m \u001b[48;5;24m\u001b[38;5;15mProfiler.start\u001b[0m  \u001b[2mpyinstrument/profiler.py:118\u001b[0m\n",
      "│  └─ \u001b[92m\u001b[2m0.000\u001b[0m [self]\u001b[0m  \u001b[2m/tmp/ipykernel_1177/3627724169.py\u001b[0m\n",
      "├─ \u001b[32m0.002\u001b[0m [self]\u001b[0m  \u001b[2mIPython/core/interactiveshell.py\u001b[0m\n",
      "├─ \u001b[92m\u001b[2m0.001\u001b[0m Bool.__get__\u001b[0m  \u001b[2mtraitlets/traitlets.py:676\u001b[0m\n",
      "│     [2 frames hidden]  \u001b[2mtraitlets\u001b[0m\n",
      "├─ \u001b[92m\u001b[2m0.001\u001b[0m helper\u001b[0m  \u001b[2mcontextlib.py:299\u001b[0m\n",
      "│     [4 frames hidden]  \u001b[2mcontextlib, <built-in>\u001b[0m\n",
      "├─ \u001b[92m\u001b[2m0.001\u001b[0m XCachingCompiler.__call__\u001b[0m  \u001b[2mcodeop.py:120\u001b[0m\n",
      "├─ \u001b[92m\u001b[2m0.001\u001b[0m _GeneratorContextManager.__enter__\u001b[0m  \u001b[2mcontextlib.py:132\u001b[0m\n",
      "└─ \u001b[92m\u001b[2m0.001\u001b[0m _GeneratorContextManager.__exit__\u001b[0m  \u001b[2mcontextlib.py:141\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyinstrument\n",
    "profiler = pyinstrument.Profiler(interval=0.000001)\n",
    "\n",
    "profiler.start()\n",
    "lmulcls(0, 0)\n",
    "profiler.stop()\n",
    "print(profiler.output_text(unicode=True, color=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 1e+03 ns, total: 17 µs\n",
      "Wall time: 38.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "0.8 * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0.3984375\n",
      "0.1 + 4 = 0.4140625\n",
      "0011111011010100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4140625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run_lmul(0.1, 4, fast=True)\n",
    "run_lmul(0.1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NumPy Inference - Predicted class: 7\n",
      "FastLmul Inference (BF16) - Predicted class: 7\n",
      "\n",
      "Output Probabilities Comparison:\n",
      "Class 0: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 1: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 2: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 3: NumPy = 0.000004, FastLmul = 0.000005, Diff = 0.000001\n",
      "Class 4: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 5: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 6: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 7: NumPy = 0.999996, FastLmul = 0.999994, Diff = 0.000001\n",
      "Class 8: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "Class 9: NumPy = 0.000000, FastLmul = 0.000000, Diff = 0.000000\n",
      "\n",
      "Both methods predicted the same class! ✓\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Type\n",
    "from hardware_accelerators.dtypes import BaseFloat, BF16\n",
    "\n",
    "\n",
    "def matrix_vector_multiply_with_lmul(\n",
    "    A: np.ndarray, v: np.ndarray, dtype: Type[BaseFloat]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform matrix-vector multiplication using FastLmul for individual multiplications\n",
    "\n",
    "    Args:\n",
    "        A: Matrix as numpy array\n",
    "        v: Vector as numpy array\n",
    "        dtype: The floating-point data type to use with FastLmul\n",
    "\n",
    "    Returns:\n",
    "        Result vector as numpy array\n",
    "    \"\"\"\n",
    "    # Initialize the FastLmul instance\n",
    "    lmul = FastLmul(dtype)\n",
    "\n",
    "    # Get dimensions\n",
    "    m, n = A.shape\n",
    "\n",
    "    # Verify that the matrix and vector can be multiplied\n",
    "    if v.shape[0] != n:\n",
    "        raise ValueError(\n",
    "            f\"Dimensions don't match for multiplication: A is {m}x{n}, v is {v.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    # Initialize result vector with zeros\n",
    "    result = np.zeros(m)\n",
    "\n",
    "    # Perform matrix-vector multiplication\n",
    "    for i in range(m):\n",
    "        sum_val = 0.0\n",
    "        for j in range(n):\n",
    "            # Use FastLmul's run method instead of regular multiplication\n",
    "            product = lmul(A[i, j], v[j])\n",
    "            # product = lmul.run(A[i, j], v[j])\n",
    "            sum_val += product\n",
    "        result[i] = sum_val\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def vector_add(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Simple vector addition without using lmul\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "\n",
    "def simulate_mlp_inference_with_lmul(\n",
    "    input_data: np.ndarray, model_weights: dict, dtype: Type[BaseFloat]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Simulate MLP inference using FastLmul for matrix multiplications\n",
    "\n",
    "    Args:\n",
    "        input_data: Input vector\n",
    "        model_weights: Dictionary containing model weights and biases\n",
    "        dtype: The floating-point data type to use with FastLmul\n",
    "\n",
    "    Returns:\n",
    "        Predicted class index\n",
    "    \"\"\"\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model_weights[\"fc1_weight\"]\n",
    "    fc1_bias = model_weights[\"fc1_bias\"]\n",
    "    fc2_weight = model_weights[\"fc2_weight\"]\n",
    "    fc2_bias = model_weights[\"fc2_bias\"]\n",
    "\n",
    "    # First layer: matrix multiplication + bias + ReLU\n",
    "    h1 = matrix_vector_multiply_with_lmul(fc1_weight, input_data, dtype)\n",
    "    h1 = vector_add(h1, fc1_bias)\n",
    "    a1 = relu(h1)\n",
    "\n",
    "    # Second layer: matrix multiplication + bias + softmax\n",
    "    h_out = matrix_vector_multiply_with_lmul(fc2_weight, a1, dtype)\n",
    "    h_out = vector_add(h_out, fc2_bias)\n",
    "    a_out = softmax(h_out)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(a_out)\n",
    "\n",
    "    return predicted_class, a_out\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    from hardware_accelerators.dtypes import BF16\n",
    "    from hardware_accelerators.nn import load_model\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(\"../models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    fc1_weight = model.fc1.weight.data.numpy()\n",
    "    fc1_bias = model.fc1.bias.data.numpy()\n",
    "    fc2_weight = model.fc2.weight.data.numpy()\n",
    "    fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "    # Store weights in a dictionary\n",
    "    model_weights = {\n",
    "        \"fc1_weight\": fc1_weight,\n",
    "        \"fc1_bias\": fc1_bias,\n",
    "        \"fc2_weight\": fc2_weight,\n",
    "        \"fc2_bias\": fc2_bias,\n",
    "    }\n",
    "\n",
    "    # Get input data (assuming get_activation() returns a sample from MNIST)\n",
    "    inputs = get_activation()\n",
    "\n",
    "    # Run inference using standard numpy operations\n",
    "    h1_numpy = inputs @ fc1_weight.T + fc1_bias\n",
    "    a1_numpy = np.maximum(0, h1_numpy)\n",
    "    h_out_numpy = a1_numpy @ fc2_weight.T + fc2_bias\n",
    "    a_out_numpy = softmax(h_out_numpy)\n",
    "    predicted_class_numpy = np.argmax(a_out_numpy)\n",
    "\n",
    "    print(f\"Standard NumPy Inference - Predicted class: {predicted_class_numpy}\")\n",
    "\n",
    "    # Run inference using FastLmul\n",
    "    predicted_class_lmul, a_out_lmul = simulate_mlp_inference_with_lmul(\n",
    "        inputs, model_weights, BF16\n",
    "    )\n",
    "\n",
    "    print(f\"FastLmul Inference (BF16) - Predicted class: {predicted_class_lmul}\")\n",
    "\n",
    "    # Compare the output probabilities\n",
    "    print(\"\\nOutput Probabilities Comparison:\")\n",
    "    for i in range(len(a_out_numpy)):\n",
    "        print(\n",
    "            f\"Class {i}: NumPy = {a_out_numpy[i]:.6f}, FastLmul = {a_out_lmul[i]:.6f}, \"\n",
    "            f\"Diff = {abs(a_out_numpy[i] - a_out_lmul[i]):.6f}\"\n",
    "        )\n",
    "\n",
    "    # Check if the predictions match\n",
    "    if predicted_class_numpy == predicted_class_lmul:\n",
    "        print(\"\\nBoth methods predicted the same class! ✓\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nPrediction mismatch: NumPy predicted {predicted_class_numpy}, \"\n",
    "            f\"FastLmul predicted {predicted_class_lmul} ✗\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "systolic_sim = SystolicArraySimulator(8, multiplier=lmul_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00762939, -0.02355957,  0.04174805, ...,  0.01428223,\n",
       "        -0.02355957,  0.0039978 ],\n",
       "       [ 0.03027344,  0.00610352,  0.04248047, ..., -0.00167847,\n",
       "         0.03173828,  0.03198242],\n",
       "       [-0.00056458,  0.01794434,  0.00424194, ...,  0.0279541 ,\n",
       "         0.01501465,  0.00738525],\n",
       "       ...,\n",
       "       [ 0.00288391, -0.02893066,  0.01831055, ..., -0.02929688,\n",
       "         0.01165771,  0.01397705],\n",
       "       [ 0.02734375, -0.01879883,  0.02734375, ...,  0.0246582 ,\n",
       "         0.02062988,  0.02722168],\n",
       "       [ 0.00787354, -0.00135803,  0.0201416 , ...,  0.02124023,\n",
       "        -0.03344727, -0.00189972]], shape=(128, 784), dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"../models/mlp_mnist_bf16.pth\")\n",
    "weights = model.fc1.weight.data.numpy(force=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n",
       "        0.15599452, 0.05808361, 0.86617615],\n",
       "       [0.60111501, 0.70807258, 0.02058449, 0.96990985, 0.83244264,\n",
       "        0.21233911, 0.18182497, 0.18340451],\n",
       "       [0.30424224, 0.52475643, 0.43194502, 0.29122914, 0.61185289,\n",
       "        0.13949386, 0.29214465, 0.36636184],\n",
       "       [0.45606998, 0.78517596, 0.19967378, 0.51423444, 0.59241457,\n",
       "        0.04645041, 0.60754485, 0.17052412],\n",
       "       [0.06505159, 0.94888554, 0.96563203, 0.80839735, 0.30461377,\n",
       "        0.09767211, 0.68423303, 0.44015249],\n",
       "       [0.12203823, 0.49517691, 0.03438852, 0.9093204 , 0.25877998,\n",
       "        0.66252228, 0.31171108, 0.52006802],\n",
       "       [0.54671028, 0.18485446, 0.96958463, 0.77513282, 0.93949894,\n",
       "        0.89482735, 0.59789998, 0.92187424],\n",
       "       [0.0884925 , 0.19598286, 0.04522729, 0.32533033, 0.38867729,\n",
       "        0.27134903, 0.82873751, 0.35675333]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "fake_acts = np.random.rand(8, 8)\n",
    "fake_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'la1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fake_acts2 \u001b[38;5;241m=\u001b[39m \u001b[43mla1\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:\u001b[38;5;241m8\u001b[39m, :\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m      3\u001b[0m gt \u001b[38;5;241m=\u001b[39m weights[:\u001b[38;5;241m8\u001b[39m, :\u001b[38;5;241m8\u001b[39m] \u001b[38;5;241m@\u001b[39m fake_acts2\n\u001b[1;32m      4\u001b[0m sim_result \u001b[38;5;241m=\u001b[39m SystolicArraySimulator\u001b[38;5;241m.\u001b[39mmatrix_multiply(\n\u001b[1;32m      5\u001b[0m     weights[:\u001b[38;5;241m8\u001b[39m, :\u001b[38;5;241m8\u001b[39m], fake_acts, multiplier\u001b[38;5;241m=\u001b[39mlmul_fast\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'la1' is not defined"
     ]
    }
   ],
   "source": [
    "fake_acts2 = la1.copy().reshape(8, -1)[:8, :8]\n",
    "\n",
    "gt = weights[:8, :8] @ fake_acts2\n",
    "sim_result = SystolicArraySimulator.matrix_multiply(\n",
    "    weights[:8, :8], fake_acts, multiplier=lmul_fast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.034  0.21   0.02   0.387 -0.132  0.245 -0.062 -0.159]\n",
      " [ 0.063  0.223  0.325  0.617  0.272  1.126  0.309  0.252]\n",
      " [-0.011 -0.018 -0.057 -0.392 -0.106 -0.305 -0.002  0.036]\n",
      " [ 0.009 -0.101 -0.209 -0.316 -0.246 -0.767 -0.445 -0.234]\n",
      " [ 0.009  0.081 -0.042  0.041 -0.183 -0.232 -0.001  0.443]\n",
      " [-0.056  0.039 -0.207 -0.223 -0.31  -0.809  0.019 -0.014]\n",
      " [ 0.008  0.105  0.265  0.17   0.418  0.516  0.368  0.396]\n",
      " [-0.062 -0.003 -0.212 -0.596 -0.282 -0.794 -0.058 -0.248]] \n",
      "\n",
      "[[ 0.013 -0.004  0.013 -0.014  0.03   0.009  0.027  0.013]\n",
      " [ 0.061  0.138  0.093  0.129  0.1    0.056  0.102  0.102]\n",
      " [-0.008 -0.007 -0.015 -0.014 -0.014 -0.019 -0.023 -0.02 ]\n",
      " [-0.068 -0.16  -0.068 -0.143 -0.09  -0.03  -0.06  -0.078]\n",
      " [-0.006  0.019  0.071  0.013  0.008  0.009  0.032  0.028]\n",
      " [-0.016 -0.048  0.004 -0.05  -0.029 -0.019 -0.047 -0.02 ]\n",
      " [ 0.034  0.106  0.096  0.118  0.055  0.06   0.054  0.098]\n",
      " [-0.02  -0.058 -0.056 -0.074 -0.046 -0.049 -0.081 -0.059]] \n",
      "\n",
      "[[False False  True False False False False False]\n",
      " [ True False False False False False False False]\n",
      " [ True False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False  True]\n",
      " [False  True False False False False False False]\n",
      " [False False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array2string(gt, precision=3, suppress_small=True), \"\\n\")\n",
    "print(np.array2string(sim_result, precision=3, suppress_small=True), \"\\n\")\n",
    "print(np.isclose(gt, sim_result, atol=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulating lmul with pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9436798631220628e-38, 0.0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, B = 3.182, 0\n",
    "\n",
    "# dtype = torch.bfloat16\n",
    "dtype = torch.float32\n",
    "\n",
    "\n",
    "def lmul(a, b, dtype):\n",
    "    torch_viewmap = {\n",
    "        torch.float32: torch.uint32,\n",
    "        torch.bfloat16: torch.uint16,\n",
    "        torch.float16: torch.uint16,\n",
    "    }\n",
    "\n",
    "    lmul_offset = {\n",
    "        torch.bfloat16: 16248,\n",
    "        torch.float32: 1064828928,\n",
    "    }\n",
    "\n",
    "    a = torch.tensor(a, dtype=dtype).view(torch_viewmap[dtype]).item()\n",
    "    b = torch.tensor(b, dtype=dtype).view(torch_viewmap[dtype]).item()\n",
    "\n",
    "    lmul_ab = a + b - lmul_offset[dtype]\n",
    "    # print(format(lmul_ab, f'0{torch_viewmap[dtype].itemsize * 8}b'))\n",
    "    lmul_ab = torch.tensor(lmul_ab, dtype=torch_viewmap[dtype]).view(dtype).item()\n",
    "\n",
    "    return lmul_ab\n",
    "\n",
    "\n",
    "lmul(A, B, dtype), A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064828928"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_combined_offset(8, 23, twos_comp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self.dim() cannot be 0 to view Float as UInt16 (different element sizes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_viewmap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self.dim() cannot be 0 to view Float as UInt16 (different element sizes)"
     ]
    }
   ],
   "source": [
    "torch.scalar_tensor(A).view(torch_viewmap[dtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate product:\n",
      " tensor([[ 0.8586,  0.4696, -1.8520,  0.0495],\n",
      "        [ 1.7733, -0.9895,  0.7423, -4.1074]])\n",
      "Exact product:\n",
      " tensor([[ 0.9098,  0.4175, -1.8140,  0.0773],\n",
      "        [ 1.7574, -1.0393,  0.7893, -4.1009]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def lmul_matmul(A: torch.Tensor, B: torch.Tensor, dtype=torch.float32):\n",
    "    if dtype == torch.float32:\n",
    "        # Use uint32 view, but cast to int64 for arithmetic\n",
    "        A_int = A.view(torch.uint32).to(torch.int64)\n",
    "        B_int = B.view(torch.uint32).to(torch.int64)\n",
    "        offset = 1064828928  # special offset for float32\n",
    "    elif dtype == torch.bfloat16:\n",
    "        A_int = A.view(torch.uint16).to(torch.int64)\n",
    "        B_int = B.view(torch.uint16).to(torch.int64)\n",
    "        offset = 16248  # special offset for bfloat16\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dtype\")\n",
    "\n",
    "    # Suppose A is (m, n) and B is (n, p)\n",
    "    # Expand dimensions so we can broadcast:\n",
    "    # A_int -> (m, n, 1) and B_int -> (1, n, p)\n",
    "    prod_int = A_int.unsqueeze(2) + B_int.unsqueeze(0) - offset  # shape: (m, n, p)\n",
    "\n",
    "    # Convert the integer results back to floating point:\n",
    "    if dtype == torch.float32:\n",
    "        prod = prod_int.to(torch.uint32).view(torch.float32)\n",
    "    else:  # bfloat16 case\n",
    "        prod = prod_int.to(torch.uint16).view(torch.bfloat16)\n",
    "\n",
    "    # Sum over the reduction dimension to complete the dot product\n",
    "    return prod.sum(dim=1)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "m, n, p = 2, 3, 4  # for instance\n",
    "A = torch.randn(m, n, dtype=torch.float32)\n",
    "B = torch.randn(n, p, dtype=torch.float32)\n",
    "C = lmul_matmul(A, B, dtype=torch.float32)\n",
    "print(\"Approximate product:\\n\", C)\n",
    "\n",
    "# Verify with exact product\n",
    "exact_product = torch.matmul(A, B)\n",
    "print(\"Exact product:\\n\", exact_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading mnist data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation: convert images to tensor and normalize them\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "# Download MNIST test data\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    batch, labels = next(iter(loader))\n",
    "    return batch.reshape(batch_size, -1).numpy(), labels.numpy()\n",
    "\n",
    "\n",
    "def get_activation():\n",
    "    image, _ = next(iter(test_loader))\n",
    "    image = image.detach().numpy().reshape(-1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"../models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "fc1_weight = model.fc1.weight.data.numpy()\n",
    "fc1_bias = model.fc1.bias.data.numpy()\n",
    "fc2_weight = model.fc2.weight.data.numpy()\n",
    "fc2_bias = model.fc2.bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 7\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"../models/mlp_mnist.pth\", \"cpu\")\n",
    "\n",
    "fc1_weight = model.fc1.weight.data.numpy()\n",
    "fc1_bias = model.fc1.bias.data.numpy()\n",
    "fc2_weight = model.fc2.weight.data.numpy()\n",
    "fc2_bias = model.fc2.bias.data.numpy()\n",
    "\n",
    "inputs = get_activation()\n",
    "\n",
    "h1 = inputs @ fc1_weight.T + fc1_bias\n",
    "a1 = np.maximum(0, h1)\n",
    "h_out = a1 @ fc2_weight.T + fc2_bias\n",
    "a_out = softmax(h_out)\n",
    "\n",
    "# get the index of the maximum value\n",
    "predicted_class = np.argmax(a_out)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 7\n"
     ]
    }
   ],
   "source": [
    "inputs = get_activation()\n",
    "\n",
    "h1 = inputs @ fc1_weight.T + fc1_bias\n",
    "a1 = np.maximum(0, h1)\n",
    "h_out = a1 @ fc2_weight.T + fc2_bias\n",
    "a_out = softmax(h_out)\n",
    "\n",
    "# get the index of the maximum value\n",
    "predicted_class = np.argmax(a_out)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lmul inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[inf, nan, inf, inf, nan, nan, inf, inf, nan, inf]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = get_batch(1)\n",
    "\n",
    "lh1 = (\n",
    "    lmul_matmul(\n",
    "        torch.tensor(inputs, dtype=torch.float32),\n",
    "        torch.tensor(fc1_weight.T, dtype=torch.float32),\n",
    "        torch.float32,\n",
    "    ).numpy()\n",
    "    + fc1_bias\n",
    ")\n",
    "la1 = np.maximum(0, lh1)\n",
    "lh_out = (\n",
    "    lmul_matmul(\n",
    "        torch.tensor(la1, dtype=torch.float32),\n",
    "        torch.tensor(fc2_weight.T, dtype=torch.float32),\n",
    "        torch.float32,\n",
    "    ).numpy()\n",
    "    + fc2_bias\n",
    ")\n",
    "# la_out = softmax(lh_out)\n",
    "\n",
    "# # get the index of the maximum value\n",
    "# predicted_class = np.argmax(la_out)\n",
    "# print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "lh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[inf, nan, inf, inf, nan, nan, inf, inf, nan, inf]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmul_matmul(\n",
    "    torch.tensor(la1, dtype=torch.float32),\n",
    "    torch.tensor(fc2_weight.T, dtype=torch.float32),\n",
    "    torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  3.1820114 ,  0.        ,  0.        ,\n",
       "         6.688619  ,  6.5706654 ,  0.        ,  0.        ,  2.4044447 ,\n",
       "         3.5749934 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.9750876 ,  0.        ,  0.        ,\n",
       "         0.        ,  4.382746  ,  7.232212  ,  0.        ,  0.        ,\n",
       "         0.        ,  2.6718044 ,  0.        ,  4.2498627 ,  0.30033502,\n",
       "         1.1679474 ,  0.        ,  1.2926131 ,  0.        ,  0.        ,\n",
       "         5.2734685 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         9.827601  ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  9.037512  ,  0.        ,  7.0829687 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.66314924,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.31634068,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  8.314439  ,  2.2188861 ,  0.892233  ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.8175012 ,\n",
       "         0.        ,  0.        ,  2.8819642 ,  2.4174857 ,  9.465485  ,\n",
       "         7.8511505 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  4.834473  , 11.010447  ,  0.        ,  0.5549504 ,\n",
       "         4.236378  ,  0.        ,  2.334083  ,  0.        , 11.079065  ,\n",
       "         0.        ,  0.        ,  3.9959395 ,  0.        ,  0.97983366,\n",
       "         7.002312  ,  0.        ,  1.5550461 ,  0.        ,  0.61436313,\n",
       "         0.        ,  7.032563  ,  2.081117  ,  0.        ,  4.3926105 ,\n",
       "         0.        ,  0.        , 12.355655  ,  1.0077223 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.8280532 ,\n",
       "         0.        ,  0.        ,  7.8808546 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0.0\n",
      "0.0 + -0.06308482587337494 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.063\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.06475932896137238 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.065\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.19324742257595062\n",
      "3.182011365890503 + 0.06073121726512909 = 0.1996130794286728\n",
      "00111110010011000110011101011111\n",
      "3.182, 0.061\n",
      "0.193, 0.200\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.10466907918453217 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.105\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.03408030420541763 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.034\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -0.1398218870162964\n",
      "6.688619136810303 + -0.02090444788336754 = -0.13406743109226227\n",
      "10111110000010010100100011111001\n",
      "6.689, -0.021\n",
      "-0.140, -0.134\n",
      "\n",
      "Expected: 0.26932308077812195\n",
      "6.57066535949707 + 0.04098870977759361 = 0.2542012631893158\n",
      "00111110100000100010011010101011\n",
      "6.571, 0.041\n",
      "0.269, 0.254\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.03825875744223595 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.038\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.20498262345790863 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.205\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.10665006190538406\n",
      "2.404444694519043 + 0.04435538128018379 = 0.10525590926408768\n",
      "00111101110101111001000001101001\n",
      "2.404, 0.044\n",
      "0.107, 0.105\n",
      "\n",
      "Expected: 0.07228610664606094\n",
      "3.574993371963501 + 0.02021992765367031 = 0.07150450348854065\n",
      "00111101100100100111000011110100\n",
      "3.575, 0.020\n",
      "0.072, 0.072\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.28701046109199524 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.287\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.05877947062253952 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.059\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.10525525361299515 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.105\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.11573610454797745 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.116\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.1170048713684082 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.117\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.14012764394283295 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.140\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.05500942841172218 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.055\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.07371222227811813 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.074\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.046562470495700836 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.047\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.04630583897233009 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.046\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.3208003640174866 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.321\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.005836934316903353\n",
      "3.9750876426696777 + 0.0014683788176625967 = 0.006068998947739601\n",
      "00111011110001101101111001110100\n",
      "3.975, 0.001\n",
      "0.006, 0.006\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.05872073769569397 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.059\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.0945957750082016 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.095\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.17947566509246826 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.179\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -1.4950904846191406\n",
      "4.38274621963501 + -0.34113097190856934 = -1.5227104425430298\n",
      "10111111110000101110100000101101\n",
      "4.383, -0.341\n",
      "-1.495, -1.523\n",
      "\n",
      "Expected: -0.24338527023792267\n",
      "7.232212066650391 + -0.03365295007824898 = -0.24343092739582062\n",
      "10111110011110010100010111110101\n",
      "7.232, -0.034\n",
      "-0.243, -0.243\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.11841452121734619 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.118\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.1512671858072281 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.151\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.1273704618215561 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.127\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.434952050447464\n",
      "2.671804428100586 + 0.16279336810112 = 0.42518728971481323\n",
      "00111110110110011011001000100110\n",
      "2.672, 0.163\n",
      "0.435, 0.425\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.11522886902093887 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.115\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.012207589112222195\n",
      "4.2498626708984375 + 0.0028724668081849813 = 0.012466161511838436\n",
      "00111100010011000011111011011111\n",
      "4.250, 0.003\n",
      "0.012, 0.012\n",
      "\n",
      "Expected: -0.08919098973274231\n",
      "0.30033501982688904 + -0.2969716489315033 = -0.09073291718959808\n",
      "10111101101110011101001000101110\n",
      "0.300, -0.297\n",
      "-0.089, -0.091\n",
      "\n",
      "Expected: -0.16944757103919983\n",
      "1.1679474115371704 + -0.1450815051794052 = -0.1738874316215515\n",
      "10111110001100100000111110001100\n",
      "1.168, -0.145\n",
      "-0.169, -0.174\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.0014181931037455797 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.001\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.12989436089992523\n",
      "1.29261314868927 + 0.10048973560333252 = 0.1226843073964119\n",
      "00111101111110110100000111101001\n",
      "1.293, 0.100\n",
      "0.130, 0.123\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.1292170286178589 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.129\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.11992790549993515 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.120\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -0.6201448440551758\n",
      "5.273468494415283 + -0.11759714782238007 = -0.6312107443809509\n",
      "10111111001000011001011100000111\n",
      "5.273, -0.118\n",
      "-0.620, -0.631\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.20351167023181915 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.204\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.2067321538925171 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.207\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.11807681620121002 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.118\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.04050401970744133 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.041\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -1.1722354888916016\n",
      "9.827601432800293 + -0.11927992105484009 = -1.199428915977478\n",
      "10111111100110011000011011100011\n",
      "9.828, -0.119\n",
      "-1.172, -1.199\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.09743411839008331 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.097\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.1632920354604721 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.163\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.1216554045677185 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.122\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.03388650342822075 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.034\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.039392467588186264 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.039\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.7134957313537598\n",
      "9.037511825561523 + -0.0789482519030571 = -0.727680504322052\n",
      "10111111001110100100100101000101\n",
      "9.038, -0.079\n",
      "-0.713, -0.728\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.14086714386940002 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.141\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.8459233641624451\n",
      "7.082968711853027 + 0.11943062394857407 = 0.872066080570221\n",
      "00111111010111110011111110111001\n",
      "7.083, 0.119\n",
      "0.846, 0.872\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.22037136554718018 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.220\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.09775752574205399 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.098\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.030066264793276787 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.030\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.016858400776982307 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.017\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.039377570152282715 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.039\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.35895052552223206 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.359\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.10042884200811386\n",
      "0.6631492376327515 + 0.151442289352417 = 0.10002104938030243\n",
      "00111101110011001101011111010110\n",
      "0.663, 0.151\n",
      "0.100, 0.100\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.014906900934875011 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.015\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.00016182483523152769 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.000\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.019313931465148926 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.019\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.05411764606833458 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.054\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.03897913172841072\n",
      "0.31634068489074707 + 0.123218834400177 = 0.040605127811431885\n",
      "00111101001001100101000110010000\n",
      "0.316, 0.123\n",
      "0.039, 0.041\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.19911056756973267 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.199\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.14216890931129456 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.142\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.044621024280786514 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.045\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.20383182168006897 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.204\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.2638700604438782 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.264\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.07828206568956375\n",
      "8.314438819885254 + -0.00941519532352686 = -0.08168436586856842\n",
      "10111101101001110100101000100010\n",
      "8.314, -0.009\n",
      "-0.078, -0.082\n",
      "\n",
      "Expected: -0.940466582775116\n",
      "2.218886137008667 + -0.4238462448120117 = -0.9336640238761902\n",
      "10111111011011110000010010011011\n",
      "2.219, -0.424\n",
      "-0.940, -0.934\n",
      "\n",
      "Expected: -0.04358125850558281\n",
      "0.8922330141067505 + -0.04884515330195427 = -0.044062841683626175\n",
      "10111101001101000111101100111101\n",
      "0.892, -0.049\n",
      "-0.044, -0.044\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.06462346762418747 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.065\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.0318794846534729 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.032\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.04737556353211403 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.047\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.09301291406154633 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.093\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.07903172075748444 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.079\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.1112174540758133\n",
      "0.8175011873245239 + -0.13604561984539032 = -0.11161670833826065\n",
      "10111101111001001001011101001101\n",
      "0.818, -0.136\n",
      "-0.111, -0.112\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.2663809061050415 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.266\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.07282588630914688 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.073\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.2969527840614319\n",
      "2.8819642066955566 + 0.10303833335638046 = 0.28802385926246643\n",
      "00111110100100110111011111011101\n",
      "2.882, 0.103\n",
      "0.297, 0.288\n",
      "\n",
      "Expected: -0.2764590382575989\n",
      "2.4174857139587402 + -0.114358089864254 = -0.2752430737018585\n",
      "10111110100011001110110010101001\n",
      "2.417, -0.114\n",
      "-0.276, -0.275\n",
      "\n",
      "Expected: 0.5108968019485474\n",
      "9.465484619140625 + 0.05397471413016319 = 0.4932191073894501\n",
      "00111110111111001000011100110111\n",
      "9.465, 0.054\n",
      "0.511, 0.493\n",
      "\n",
      "Expected: -1.6057499647140503\n",
      "7.8511505126953125 + -0.20452415943145752 = -1.6614809036254883\n",
      "10111111110101001010101101101000\n",
      "7.851, -0.205\n",
      "-1.606, -1.661\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.03845495730638504 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.038\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.04659215360879898 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.047\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.08627378195524216 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.086\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.4743741750717163 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.474\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.28316882252693176 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.283\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.6273109912872314\n",
      "4.834473133087158 + 0.12975788116455078 = 0.6545906662940979\n",
      "00111111001001111001001101000001\n",
      "4.834, 0.130\n",
      "0.627, 0.655\n",
      "\n",
      "Expected: -0.15551961958408356\n",
      "11.010446548461914 + -0.0141247333958745 = -0.1558464616537094\n",
      "10111110000111111001011000110111\n",
      "11.010, -0.014\n",
      "-0.156, -0.156\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.04679657146334648 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.047\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.09121441841125488\n",
      "0.5549504160881042 + -0.1643649935722351 = -0.09295754879713058\n",
      "10111101101111100110000010000111\n",
      "0.555, -0.164\n",
      "-0.091, -0.093\n",
      "\n",
      "Expected: 0.6012399792671204\n",
      "4.236378192901611 + 0.14192311465740204 = 0.6284897327423096\n",
      "00111111001000001110010010110100\n",
      "4.236, 0.142\n",
      "0.601, 0.628\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.1360066831111908 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.136\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -0.05322319641709328\n",
      "2.334083080291748 + -0.02280261367559433 = -0.05277840048074722\n",
      "10111101010110000010111000101010\n",
      "2.334, -0.023\n",
      "-0.053, -0.053\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.05957978591322899 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.060\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 1.4121906757354736\n",
      "11.079065322875977 + 0.12746478617191315 = 1.4671014547348022\n",
      "00111111101110111100100111111011\n",
      "11.079, 0.127\n",
      "1.412, 1.467\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.2783943712711334 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.278\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.263845831155777 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.264\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.9020017385482788\n",
      "3.9959394931793213 + -0.2257295846939087 = -0.9331532120704651\n",
      "10111111011011101110001100100001\n",
      "3.996, -0.226\n",
      "-0.902, -0.933\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.13626030087471008 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.136\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -0.21115952730178833\n",
      "0.9798336625099182 + -0.2155054807662964 = -0.21827639639377594\n",
      "10111110010111111000001111011001\n",
      "0.980, -0.216\n",
      "-0.211, -0.218\n",
      "\n",
      "Expected: 1.1012144088745117\n",
      "7.002312183380127 + 0.15726439654827118 = 1.0711932182312012\n",
      "00111111100010010001110011011100\n",
      "7.002, 0.157\n",
      "1.101, 1.071\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.22908782958984375 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.229\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.12296853959560394\n",
      "1.5550460815429688 + -0.0790771022439003 = -0.11767373234033585\n",
      "10111101111100001111111011101101\n",
      "1.555, -0.079\n",
      "-0.123, -0.118\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.10032489895820618 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.100\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.05358322337269783\n",
      "0.6143631339073181 + 0.08721750974655151 = 0.05270957574248314\n",
      "00111101010101111110010111111111\n",
      "0.614, 0.087\n",
      "0.054, 0.053\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.03236164525151253 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.032\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -2.490602970123291\n",
      "7.032563209533691 + -0.35415294766426086 = -2.4745051860809326\n",
      "11000000000111100101111001001011\n",
      "7.033, -0.354\n",
      "-2.491, -2.475\n",
      "\n",
      "Expected: 0.2299189418554306\n",
      "2.0811169147491455 + 0.11047862470149994 = 0.23383955657482147\n",
      "00111110011011110111001110100011\n",
      "2.081, 0.110\n",
      "0.230, 0.234\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.14510954916477203 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.145\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.16927942633628845\n",
      "4.392610549926758 + 0.038537316024303436 = 0.17423084378242493\n",
      "00111110001100100110100110010010\n",
      "4.393, 0.039\n",
      "0.169, 0.174\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.07889293879270554 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.079\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.12466131150722504 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.125\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: -0.06612978130578995\n",
      "12.3556547164917 + -0.0053521874360740185 = -0.06178490072488785\n",
      "10111101011111010001001000101010\n",
      "12.356, -0.005\n",
      "-0.066, -0.062\n",
      "\n",
      "Expected: -0.16920040547847748\n",
      "1.00772225856781 + -0.1679038107395172 = -0.17668159306049347\n",
      "10111110001101001110110000000101\n",
      "1.008, -0.168\n",
      "-0.169, -0.177\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.13672581315040588 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.137\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.19655142724514008 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.197\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.04905140399932861 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.049\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.11708090454339981 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.117\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.18916283547878265 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.189\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: -0.7089332938194275\n",
      "2.8280532360076904 + -0.2506788969039917 = -0.739621102809906\n",
      "10111111001111010101011111001111\n",
      "2.828, -0.251\n",
      "-0.709, -0.740\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + -0.04226519167423248 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, -0.042\n",
      "-0.000, 0.000\n",
      "\n",
      "Expected: 0.0\n",
      "0.0 + 0.04531633108854294 = 0.0\n",
      "00000000000000000000000000000000\n",
      "0.000, 0.045\n",
      "0.000, 0.000\n",
      "\n",
      "Expected: 0.288237988948822\n",
      "7.880854606628418 + 0.036574456840753555 = 0.30077406764030457\n",
      "00111110100110011111111100001111\n",
      "7.881, 0.037\n",
      "0.288, 0.301\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(-5.1689677), -5.280333912931383)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h20 = 0\n",
    "lh20 = 0\n",
    "for a, w in zip(la1[0], fc2_weight[0]):\n",
    "    product = a * w\n",
    "    lproduct = run_lmul(a, w, Float32)\n",
    "    print(f\"{a:.3f}, {w:.3f}\")\n",
    "    print(f\"{product:.3f}, {lproduct:.3f}\\n\")\n",
    "    h20 += product\n",
    "    lh20 += lproduct\n",
    "h20, lh20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'exponent_bits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.47\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/private/dsc180a/hardware-accelerators/hardware_accelerators/rtllib/lmul.py:12\u001b[0m, in \u001b[0;36mlmul\u001b[0;34m(float_a, float_b, dtype, fast)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlmul\u001b[39m(float_a: WireVector, float_b: WireVector, dtype: Type[BaseFloat], fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m     e_bits, m_bits \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexponent_bits\u001b[49m(), dtype\u001b[38;5;241m.\u001b[39mmantissa_bits()\n\u001b[1;32m     13\u001b[0m     em_bits \u001b[38;5;241m=\u001b[39m e_bits \u001b[38;5;241m+\u001b[39m m_bits\n\u001b[1;32m     14\u001b[0m     sign_a \u001b[38;5;241m=\u001b[39m float_a[em_bits]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'exponent_bits'"
     ]
    }
   ],
   "source": [
    "lmul(0, 0.47, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01075647, -0.14156555,  0.01885387, -0.1005828 ,  0.08457243,\n",
       "       -0.13470922, -0.09962139, -0.13597849,  0.15926038,  0.0367002 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.2412014, -12.86712  ,  -1.4991012,   1.0043828, -17.54349  ,\n",
       "        -8.378266 , -18.727932 ,  13.432735 ,  -7.506784 ,  -3.5312648],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'exponent_bits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(fc2_weight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m         output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mla1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc2_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     final_output\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m      7\u001b[0m final_output\n",
      "File \u001b[0;32m~/private/dsc180a/hardware-accelerators/hardware_accelerators/rtllib/lmul.py:12\u001b[0m, in \u001b[0;36mlmul\u001b[0;34m(float_a, float_b, dtype, fast)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlmul\u001b[39m(float_a: WireVector, float_b: WireVector, dtype: Type[BaseFloat], fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m     e_bits, m_bits \u001b[38;5;241m=\u001b[39m \u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexponent_bits\u001b[49m(), dtype\u001b[38;5;241m.\u001b[39mmantissa_bits()\n\u001b[1;32m     13\u001b[0m     em_bits \u001b[38;5;241m=\u001b[39m e_bits \u001b[38;5;241m+\u001b[39m m_bits\n\u001b[1;32m     14\u001b[0m     sign_a \u001b[38;5;241m=\u001b[39m float_a[em_bits]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'exponent_bits'"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "for col in range(fc2_weight.shape[0]):\n",
    "    output = 0\n",
    "    for row in range(fc2_weight.shape[1]):\n",
    "        output += lmul(la1[0, row], fc2_weight.T[row, col], torch.float32)\n",
    "    final_output.append(output)\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
