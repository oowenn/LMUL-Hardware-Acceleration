{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1026b0ec-96d6-4428-a06b-c5089d94569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "def lmul_bits(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    #THIS IS THE VERSION THAT IS USED FOR THIS NOTEBOOK. it is based on the definition in the section 2 of the report\n",
    "    \"\"\"\n",
    "    goal should be:\n",
    "    two tensors of floats:\n",
    "    Convert to bfloat16 bit representation\n",
    "    Extract sign and field bits (exponent+mantissa)\n",
    "    Add fields with bias compensation\n",
    "    Handle overflow/underflow\n",
    "    XOR signs\n",
    "    Repack into BF16 bits and convert back to float32 for readability\n",
    "\n",
    "    Except that we add like this bit shift at the end \n",
    "    result = result + (result / (1 << 5)) + (result / (1 << 6))\n",
    "    because for some reason it's been off by like 5% (and this corrects that)\n",
    "    you can remove that line if you want to measure the raw mult accuracy between torch tensors\n",
    "      and write about that in the report (iirc it should be a further 10% accuracy drop though).\n",
    "      It seems that for LSTM, because multiplication errors stack up a lot quicker the actual raw \n",
    "      mult. result should matter a lot more (as opposed to MLP which just cares about being in \n",
    "      ball park (relative) since its only like 2 mult operations)\n",
    "    \"\"\"\n",
    "    #Convert to BF16 binary form (simulate using float32)\n",
    "    a = a.to(torch.float32)\n",
    "    b = b.to(torch.float32)\n",
    "    #view raw bit patterns\n",
    "    a_bits = a.view(torch.int32)\n",
    "    b_bits = b.view(torch.int32)\n",
    "    #Extract BF16 upper bits (simulate truncation)\n",
    "    a_bf16 = (a_bits >> 16) & 0xFFFF\n",
    "    b_bf16 = (b_bits >> 16) & 0xFFFF\n",
    "    #Extract sign and field bits ---\n",
    "    a_sign = (a_bf16 >> 15) & 0x1\n",
    "    b_sign = (b_bf16 >> 15) & 0x1\n",
    "    a_field = a_bf16 & 0x7FFF\n",
    "    b_field = b_bf16 & 0x7FFF\n",
    "    #Handle zeros / subnormals (exp == 0) ---\n",
    "    a_exp = (a_field >> 7) & 0xFF\n",
    "    b_exp = (b_field >> 7) & 0xFF\n",
    "    zero_mask = (a_exp == 0) | (b_exp == 0)\n",
    "    #Add the fields with bias correction ---\n",
    "    OFFSET_MOD = 0x4080  # (2^15 - (127 << 7)) & 0x7FFF = 0x4080\n",
    "    sum_full = a_field.to(torch.int32) + b_field.to(torch.int32) + OFFSET_MOD\n",
    "    #use 17-bit precision\n",
    "    carry2 = (sum_full >> 15) & 0x3  # top 2 bits\n",
    "    field_sel = torch.zeros_like(sum_full)\n",
    "    #Handle overflow/underflow\n",
    "    # 00: underflow → 0\n",
    "    mask_underflow = (carry2 == 0)\n",
    "    # 01: normal → use lower 15 bits\n",
    "    mask_normal = (carry2 == 1)\n",
    "    # 1x: overflow → saturate to 0x7FFF\n",
    "    mask_overflow = (carry2 >= 2)\n",
    "\n",
    "    field_sel = torch.where(mask_normal, sum_full & 0x7FFF, field_sel)\n",
    "    field_sel = torch.where(mask_overflow, torch.tensor(0x7FFF, dtype=torch.int32, device=sum_full.device), field_sel)\n",
    "\n",
    "    #Calculate the sign bit\n",
    "    s_result = (a_sign ^ b_sign).to(torch.int32)\n",
    "    s_result = torch.where(field_sel == 0, torch.tensor(0, device=sum_full.device, dtype=torch.int32), s_result)\n",
    "    #Pack result (sign << 15) | field\n",
    "    result_bits_bf16 = ((s_result << 15) | field_sel).to(torch.int32)\n",
    "\n",
    "    \n",
    "    #Convert back to float32 by restoring 16 LSBs as zeros\n",
    "    result_bits_f32 = result_bits_bf16 << 16\n",
    "    result = result_bits_f32.view(torch.float32)\n",
    "    #bias bitshift\n",
    "    result = result + (result / (1 << 5)) + (result / (1 << 6))\n",
    "    #Handle zeros\n",
    "    result = torch.where(zero_mask, torch.zeros_like(result), result)\n",
    "   \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fd5a1-ef9d-4a2e-b8de-5d33d2baa2b5",
   "metadata": {},
   "source": [
    "## Quick note\n",
    "\n",
    "In the lmul_bits defined above, there is a 1.05 multiplier (5%) bias applied at the end of the function.\n",
    "This is to normalize the mean magnitude of L-Mul outputs to match true multiplication! \n",
    "(Note that its actually a bit shift that results in +4.69% which is close enough). \n",
    "\n",
    "# What about the LMUL in the rtl folder?\n",
    "\n",
    "It takes ages to run a pure python non torch tensorized function in an LSTM (which is why lmul_bits was defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f19d4-aa94-4b4a-b574-eb1945dd738d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LSTM accuracy: 97.49%\n",
      "L-Mul LSTM accuracy: 97.71%\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "def lmul(a, b, M=7):\n",
    "    #can add some internal logic here, but this is just a blank call to the lmul bits defined in the first cell\n",
    "    return lmul_bits(a, b)\n",
    "\n",
    "#similar logic to the MLP version\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.M = M\n",
    "        #Concatenated weight matrices (pretty sure this is the default way to do it)\n",
    "        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        combined = torch.cat((x_t, h_prev), dim=1)  # [B, I+H]\n",
    "\n",
    "        if self.use_lmul:\n",
    "            #manually replace the matmul in Linear with LMul IF enabled\n",
    "            W = self.W.weight\n",
    "            b = self.W.bias\n",
    "            B, I = combined.shape\n",
    "            O, _ = W.shape\n",
    "            x_exp = combined.unsqueeze(1)   # [B, 1, I]\n",
    "            W_exp = W.unsqueeze(0)          # [1, O, I]\n",
    "            prod = lmul(x_exp, W_exp, M=self.M)\n",
    "            gates = prod.sum(dim=2) + b\n",
    "        else:\n",
    "            gates = self.W(combined)\n",
    "\n",
    "        i, f, g, o = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        if self.use_lmul:\n",
    "            #cell update with LMul\n",
    "            c_t = lmul(f, c_prev, M=self.M) + lmul(i, g, M=self.M)\n",
    "            h_t = lmul(o, torch.tanh(c_t), M=self.M)\n",
    "        else:\n",
    "            c_t = f * c_prev + i * g\n",
    "            h_t = o * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=28, hidden_size=128, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.lstm_cell = LSTMLayer(input_size, hidden_size, use_lmul, M)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, 28, 28] (treat each row as a timestep)\n",
    "        B = x.size(0)\n",
    "        x = x.squeeze(1)  # [B, 28, 28]\n",
    "        h = torch.zeros(B, self.hidden_size)\n",
    "        c = torch.zeros(B, self.hidden_size)\n",
    "        #this isn't the best way to do this; ideally you'd have some kind of feature extraction for \n",
    "        #the dim sizes but *if it works it works*\n",
    "        for t in range(28):  #sequence length = 28\n",
    "            x_t = x[:, t, :]  #each row = input vector\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "\n",
    "        out = self.fc(h)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "def train_model(model, optimizer, loader, epochs=2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += len(target)\n",
    "    return 100 * correct / total\n",
    "\n",
    "#(normal multiply)\n",
    "model = LSTMClassifier(use_lmul=False)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_model(model, opt, train_loader, epochs=5)\n",
    "baseline_acc = test_acc(model, test_loader)\n",
    "print(f\"Baseline LSTM accuracy: {baseline_acc:.2f}%\")\n",
    "\n",
    "#L-Mul arithmetic\n",
    "model_lmul = LSTMClassifier(use_lmul=True, M=7)\n",
    "model_lmul.load_state_dict(model.state_dict())\n",
    "lmul_acc = test_acc(model_lmul, test_loader)\n",
    "print(f\"L-Mul LSTM accuracy: {lmul_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b403baba-ab1a-418a-97ea-a334654412d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25.1250])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([5])\n",
    "b = torch.tensor([5])\n",
    "lmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6204dd5-d2fa-4971-93a7-ba4cfa6d2fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(|LMul|)/mean(|mul|) = 1.0016238620280844\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(10000000)\n",
    "b = torch.randn(10000000)\n",
    "prod = a * b\n",
    "approx = lmul(a, b)\n",
    "print(\"mean(|LMul|)/mean(|mul|) =\", approx.abs().mean().item() / prod.abs().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e70ac4-47c8-4e2e-bc10-047a3709a436",
   "metadata": {},
   "source": [
    "## What is Fashion-MNIST?\n",
    "\n",
    "Fashion-MNIST is a dataset of 28×28 grayscale images, just like MNIST digits, but instead of handwritten digits, each image is a clothing or accessory item from Zalando.\n",
    "\n",
    "It was created because MNIST was too easy, as even our models can get high accuracy on MNIST, which isn’t useful for benchmarking anymore. Fashion-MNIST provides a similar format but more challenging, without being huge.\n",
    "\n",
    "Let's see if there's an accuracy difference that appears! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ba8555-655b-401b-895c-dc2365e93e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LSTM accuracy: 86.39%\n",
      "L-Mul LSTM accuracy: 84.97%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '.', train=True, download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '.', train=False, download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "    ),\n",
    "    batch_size=128, shuffle=False\n",
    ")\n",
    "\n",
    "def lmul(a, b, M=7):\n",
    "    #can add some internal logic here, but this is just a blank call to the lmul bits defined in the first cell\n",
    "    return lmul_bits(a, b)\n",
    "\n",
    "#similar logic to the MLP version\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.M = M\n",
    "        #Concatenated weight matrices (pretty sure this is the default way to do it)\n",
    "        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        combined = torch.cat((x_t, h_prev), dim=1)  # [B, I+H]\n",
    "\n",
    "        if self.use_lmul:\n",
    "            #manually replace the matmul in Linear with LMul IF enabled\n",
    "            W = self.W.weight\n",
    "            b = self.W.bias\n",
    "            B, I = combined.shape\n",
    "            O, _ = W.shape\n",
    "            x_exp = combined.unsqueeze(1)   # [B, 1, I]\n",
    "            W_exp = W.unsqueeze(0)          # [1, O, I]\n",
    "            prod = lmul(x_exp, W_exp, M=self.M)\n",
    "            gates = prod.sum(dim=2) + b\n",
    "        else:\n",
    "            gates = self.W(combined)\n",
    "\n",
    "        i, f, g, o = torch.chunk(gates, 4, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        if self.use_lmul:\n",
    "            #cell update with LMul\n",
    "            c_t = lmul(f, c_prev, M=self.M) + lmul(i, g, M=self.M)\n",
    "            h_t = lmul(o, torch.tanh(c_t), M=self.M)\n",
    "        else:\n",
    "            c_t = f * c_prev + i * g\n",
    "            h_t = o * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=28, hidden_size=128, use_lmul=False, M=7):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lmul = use_lmul\n",
    "        self.lstm_cell = LSTMLayer(input_size, hidden_size, use_lmul, M)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 1, 28, 28] (treat each row as a timestep)\n",
    "        B = x.size(0)\n",
    "        x = x.squeeze(1)  # [B, 28, 28]\n",
    "        h = torch.zeros(B, self.hidden_size)\n",
    "        c = torch.zeros(B, self.hidden_size)\n",
    "        #this isn't the best way to do this; ideally you'd have some kind of feature extraction for \n",
    "        #the dim sizes but *if it works it works*\n",
    "        for t in range(28):  #sequence length = 28\n",
    "            x_t = x[:, t, :]  #each row = input vector\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "\n",
    "        out = self.fc(h)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "def train_model(model, optimizer, loader, epochs=2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            pred = model(data).argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += len(target)\n",
    "    return 100 * correct / total\n",
    "\n",
    "#(normal multiply)\n",
    "model = LSTMClassifier(use_lmul=False)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_model(model, opt, train_loader, epochs=5)\n",
    "baseline_acc = test_acc(model, test_loader)\n",
    "print(f\"Baseline LSTM accuracy: {baseline_acc:.2f}%\")\n",
    "\n",
    "#L-Mul arithmetic\n",
    "model_lmul = LSTMClassifier(use_lmul=True, M=7)\n",
    "model_lmul.load_state_dict(model.state_dict())\n",
    "lmul_acc = test_acc(model_lmul, test_loader)\n",
    "print(f\"L-Mul LSTM accuracy: {lmul_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd7d52-8aa9-4f47-b432-12902d8ed3fa",
   "metadata": {},
   "source": [
    "## Oh no!\n",
    "\n",
    "Here we see a bit of an accuracy drop. This *is* to be expected, since LMUL is an approximate. Given that this is still one data point, however, let's do a test on other MNIST datasets.\n",
    "\n",
    "Warning: The cell below takes a long time to run (about 15x longer than the previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a7ed0f-8a33-4fb8-8b7a-c4f8ddccddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== dataset=FashionMNIST seed=0 ===\n",
      "FP32 acc: 84.610\n",
      "LMUL infer acc: 83.550\n",
      "\n",
      "=== dataset=FashionMNIST seed=1 ===\n",
      "FP32 acc: 84.340\n",
      "LMUL infer acc: 83.060\n",
      "\n",
      "=== dataset=FashionMNIST seed=2 ===\n",
      "FP32 acc: 84.200\n",
      "LMUL infer acc: 83.910\n",
      "\n",
      "=== dataset=FashionMNIST seed=3 ===\n",
      "FP32 acc: 83.600\n",
      "LMUL infer acc: 82.820\n",
      "\n",
      "=== dataset=FashionMNIST seed=4 ===\n",
      "FP32 acc: 84.730\n",
      "LMUL infer acc: 84.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== dataset=KMNIST seed=0 ===\n",
      "FP32 acc: 85.640\n",
      "LMUL infer acc: 85.320\n",
      "\n",
      "=== dataset=KMNIST seed=1 ===\n",
      "FP32 acc: 85.530\n",
      "LMUL infer acc: 85.460\n",
      "\n",
      "=== dataset=KMNIST seed=2 ===\n",
      "FP32 acc: 85.080\n",
      "LMUL infer acc: 85.290\n",
      "\n",
      "=== dataset=KMNIST seed=3 ===\n",
      "FP32 acc: 86.480\n",
      "LMUL infer acc: 86.490\n",
      "\n",
      "=== dataset=KMNIST seed=4 ===\n",
      "FP32 acc: 86.600\n",
      "LMUL infer acc: 86.600\n",
      "\n",
      "=== dataset=SeqMNIST seed=0 ===\n",
      "FP32 acc: 96.610\n",
      "LMUL infer acc: 96.400\n",
      "\n",
      "=== dataset=SeqMNIST seed=1 ===\n",
      "FP32 acc: 96.470\n",
      "LMUL infer acc: 96.550\n",
      "\n",
      "=== dataset=SeqMNIST seed=2 ===\n",
      "FP32 acc: 96.090\n",
      "LMUL infer acc: 96.180\n",
      "\n",
      "=== dataset=SeqMNIST seed=3 ===\n",
      "FP32 acc: 95.680\n",
      "LMUL infer acc: 95.470\n",
      "\n",
      "=== dataset=SeqMNIST seed=4 ===\n",
      "FP32 acc: 96.960\n",
      "LMUL infer acc: 96.920\n",
      "\n",
      "|===>SUMMARY<===|\n",
      "('FashionMNIST', 'FP32'): mean=84.30 std=0.40 n=5\n",
      "('FashionMNIST', 'LMUL_infer'): mean=83.47 std=0.47 n=5\n",
      "('KMNIST', 'FP32'): mean=85.87 std=0.58 n=5\n",
      "('KMNIST', 'LMUL_infer'): mean=85.83 std=0.59 n=5\n",
      "('SeqMNIST', 'FP32'): mean=96.36 std=0.44 n=5\n",
      "('SeqMNIST', 'LMUL_infer'): mean=96.30 std=0.48 n=5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#datasets mapping (function that returns train/test loaders); otherwise the code gets pretty long\n",
    "def get_loaders(name, batch_train=128, batch_test=1000):\n",
    "    T = transforms.ToTensor()\n",
    "    if name == 'FashionMNIST':\n",
    "        ds = datasets.FashionMNIST\n",
    "    elif name == 'KMNIST':\n",
    "        ds = datasets.KMNIST\n",
    "    elif name == 'EMNIST_letters':\n",
    "        ds = lambda root, train, download, transform: datasets.EMNIST(root, split='letters', train=train, download=download, transform=transform)\n",
    "    elif name == 'MNIST':\n",
    "        ds = datasets.MNIST\n",
    "    elif name == 'SeqMNIST':  #this will use MNIST but will flatten in model \n",
    "        ds = datasets.MNIST\n",
    "    elif name == 'CIFAR10_gray':\n",
    "        #convert to grayscale\n",
    "        T = transforms.Compose([transforms.Grayscale(), transforms.ToTensor()])\n",
    "        ds = datasets.CIFAR10\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        ds('.', train=True, download=True, transform=T),\n",
    "        batch_size=batch_train, shuffle=True\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        ds('.', train=False, download=True, transform=T),\n",
    "        batch_size=batch_test, shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "#fix seeder\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "#Experiment runner\n",
    "def run_experiment(datasets_list, seeds=[0,1,2,3,4], epochs=5, lr=1e-3):\n",
    "    results = defaultdict(list)  \n",
    "    #results[(dataset, condition)] -> list of accuracies\n",
    "\n",
    "    for ds_name in datasets_list:\n",
    "        train_loader, test_loader = get_loaders(ds_name)\n",
    "        for seed in seeds:\n",
    "            print(f\"\\n=== dataset={ds_name} seed={seed} ===\")\n",
    "            set_seed(seed)\n",
    "\n",
    "            #training FP32 baseline model\n",
    "            model_fp = LSTMClassifier(use_lmul=False).to(device)\n",
    "            opt = torch.optim.Adam(model_fp.parameters(), lr=lr)\n",
    "            train_model(model_fp, opt, train_loader, epochs=epochs)\n",
    "            acc_fp = test_acc(model_fp, test_loader)\n",
    "            print(f\"FP32 acc: {acc_fp:.3f}\")\n",
    "            results[(ds_name, 'FP32')].append(acc_fp)\n",
    "\n",
    "            #Eval same weights under LMUL inference\n",
    "            model_lm = LSTMClassifier(use_lmul=True).to(device)\n",
    "            model_lm.load_state_dict(model_fp.state_dict())  # identical weights\n",
    "            acc_lm = test_acc(model_lm, test_loader)\n",
    "            print(f\"LMUL infer acc: {acc_lm:.3f}\")\n",
    "            results[(ds_name, 'LMUL_infer')].append(acc_lm)\n",
    "\n",
    "\n",
    "\n",
    "    #basically give me the summ stats\n",
    "    summary = {}\n",
    "    for key, vals in results.items():\n",
    "        arr = np.array(vals)\n",
    "        summary[key] = (arr.mean(), arr.std(), len(arr))\n",
    "    return summary, results\n",
    "\n",
    "#u can select the other ones too but that would take too long for me to run ;;-;;\n",
    "datasets_to_run = ['FashionMNIST', 'KMNIST', 'SeqMNIST'] \n",
    "summary, raw = run_experiment(datasets_to_run, seeds=[0,1,2,3,4], epochs=3, lr=1e-3)\n",
    "print(\"\\n|===>SUMMARY<===|\")\n",
    "for k, (mu, std, n) in summary.items():\n",
    "    print(f\"{k}: mean={mu:.2f} std={std:.2f} n={n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1e245-13ad-4673-ae15-2eb7343ac81d",
   "metadata": {},
   "source": [
    "### What are these?\n",
    "\n",
    "SeqMNIST is basically the MNIST dataset, but feeds the LSTM 1 pixel per step instead of a 28 dim vector. This means that the LSTM has to 'remember' or 'integrate' information from magnitudes more steps to come up with a classification as opposed to the normal dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d6f6c-1b03-4c22-96a0-28aec254d51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
